---
author: "Oliver Speer"
date: "10.12.2024"
---

::::: columns
::: {.column width="50%"}
:::

::: {.column width="50%"}
```{r version number, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
cat("Version:\n",format(file.info("DataPreparation.qmd")$mtime,"%d. %B %Y"))
```
:::
:::::

```{r, echo=FALSE, message=FALSE, warning=FALSE}
source("StartUp.R")
StartUpRoutine()
```

# Data Preparation {.unnumbered}


## Auswahl der Daten: Auswahl der relevanten Elemente und Merkmale {#var_selection .justify}




```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(dagitty)
dag <- dagitty('dag {
  age -> gammopathy
  food -> cholesterol
  cholesterol -> gammopathy
  smoking -> gammopathy
  alcohol -> liver_enzyme
  alcohol -> cholesterol
  gammopathy <- liver_enzyme
  sex -> gammopathy
  GUS -> gammopathy
  age -> GUS
  sex -> liver_enzyme
  sex -> smoking
  sex -> alcohol
  sex -> cholesterol
  sex -> GUS
  sex -> age
  age -> liver_enzyme
  age -> food
  age -> smoking
  age -> alcohol
  
}')

plot(dag)

adjustmentSets(dag, exposure = "sex", outcome = "gammopathy")
adjustmentSets(dag, exposure = "age", outcome = "gammopathy")
```
Sollte ich ein gam oder glm modellieren, dann kann ich aus dem oberen DAG mitnehmen, dass ich für "Alter" und "Geschlecht" adjustieren sollte.

[**Q: Aber wie wähle ich für ein random forest Modell die relevanten Variablen aus?**]{style="#00008B"}



## Bereinigen der Daten {.justify}
```{r read csv, echo=T, message=FALSE, warning=FALSE}
# Einlesen der Daten

EPcsv <- read_csv2("epcdatawithcurve.csv", col_select = c(1:4, 6), col_types = "icccc") |> 
  rename_with(~ c("ID", "TaNu", "Bef.EP", "Bef.ImFix", "curve"))
#str(EPcsv)
```
Datensätze ohne Chromatogramm-Daten, und auch "SISTIERTE" und "entfernte"  Datensätze werden entfernt.<br>
Kontrolle der Daten nach dem Filtern:

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# filtern der Daten EPcsv1$curve !NA
EPcsv1 <- EPcsv[!is.na(EPcsv$curve)&!is.na(EPcsv$Bef.ImFix),] |>   
  filter(!grepl("SISTIERT|entfernt", Bef.EP, ignore.case = TRUE)) |> 
  filter(!grepl("SISTIERT|entfernt", Bef.ImFix, ignore.case = TRUE))
 
```
- Von `r sum(is.na(EPcsv$curve))` Datensätzen ohne Chromatogramm-Daten  wurden `r sum(is.na(EPcsv$curve)) - sum(is.na(EPcsv1$curve))` Datensätze entfernt.

- Von `r sum(is.na(EPcsv$Bef.ImFix))` Datensätzen ohne iFix-Befundtexte wurden `r sum(is.na(EPcsv$Bef.ImFix)) - sum(is.na(EPcsv1$Bef.ImFix))` Datensätze entfernt.

- Von `r sum(EPcsv$Bef.ImFix == "SISTIERT", na.rm = T)` sistierten iFix-Aufträgen wurden `r sum(EPcsv$Bef.ImFix == "SISTIERT", na.rm = T) - sum(EPcsv1$Bef.ImFix == "SISTIERT", na.rm = T)`   iFix-Befunde entfernt. 

- Von `r sum(EPcsv$Bef.ImFix == "entfernt", na.rm = T)` entfernten iFix-Aufträgen wurden `r sum(EPcsv$Bef.ImFix == "entfernt", na.rm = T) - sum(EPcsv1$Bef.ImFix == "entfernt", na.rm = T)`   iFix-Befunde entfernt. 

::: {.panel-tabset}
### Bereiningen der Befundtexte {.justify}
Wie im Kapitel "Data Understanding" beschrieben, werden die iFix-Befundtexte aufbereitet. Dazu werden Schreibfehler, Synonyme, Satzzeichen, Gross- und Kleinschreibung und Stopwörter bereinigt.
In den Tabsets werden die Stopwörterlisten angezeigt.

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Tokenisierung der imFix-Befundtexte
library(tidytext)
library(stringr)
library(stopwords)

# Monate als Stopwörter
monate <- c(
  "Januar", "Jan",
  "Februar", "Feb",
  "März", "März",
  "April", "Apr",
  "Mai", "Mai",
  "Juni", "Jun",
  "Juli", "Jul",
  "August", "Aug",
  "September", "Sept",
  "Oktober", "Okt",
  "November", "Nov",
  "Dezember", "Dez",
  "Monat", "Monaten", "monaten"
) 

# Stopwörterliste aus den Befunden (manuell ergänzt)
ad.string <- c("ist die quantitative Bestimmung der aus der aktuellen Serumprobe und oder eine Verlaufskontrolle zu empfehlen Homogene Unspezifischer Befund vereinbar mit einer sowie der isolierten einer äusserst diskreten und diffusen in den Spuren für sicheren Je nach klinischem Kontext ist eine Verlaufskontrolle zu empfehlen  und bedingt einen modifizierten Ansatz Das initial berichtete endogene Weiterhin schwacher des vorbekannten sowie der zusätzlichen diskreten Zonierungen in den Spuren wesentliche des stark ausgeprägten Mobilität sowie des sehr schwach ausgeprägten leicht einer äusserst diskreten und diffusen Befund mit einer schwach ausgeprägten diskret diffusen bekannten Bei unklarer Ätiologie Verlaufskontrolle diskreter Bei Ätiologie Verlaufskontrolle Typ Verlaufskontrolle B RGB initialen klin")

# stopwords anpassen, da in den tokens "nicht", "kein", "ohne" und Varianten 
# enthalten bleiben sollen
stopwords.olli <- setdiff(stopwords("de", "stopwords-iso"), 
                          c("nicht", 
                            "keine", 
                            "kein", 
                            "keinem", 
                            "keinen", 
                            "keiner", 
                            "keines", 
                            "ohne"))

stopwords.olli <- c(
                      stopwords.olli, ad.string |>
                      str_split(" ") |>
                      unlist(), monate
                    ) |>
  tolower() |>
  unique() |>
  setdiff("") |>
  sort()
```


### Stopwörterliste


```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
stopwords.olli
```

### Stopwörterliste als regex Ausdruck

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
stopwords.olli.col <- paste0("\\b(", paste(stopwords.olli, collapse = "|"), ")\\b")


(stopwords.olli.col)
```

:::

### Bereinigen der Befundtexte {.justify}
- Entfernen der Klammern, Zahlen, Satzzeichen, Gross- und Kleinschreibung, Stopwörter und überflüssiger Leerzeichen.   
-Vereinheitlichung der Synonyme von "monoklonalen" und "M-Proteinen".

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
EPcsv1 <- EPcsv1 |>
  mutate(
    Bef.ImFix_cleaned = gsub("\\([^)]*\\)", "", Bef.ImFix),
    Bef.ImFix_cleaned = gsub("[0-9]+", "", Bef.ImFix_cleaned),
    #Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, monate, ""),
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, "[[:punct:]]", " "),
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, " +", " "),
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, "\\bmonoklonale?n?s?\\b", "M"),
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, "\\bM Proteine?n?s?\\b", "MProtein"),
    Bef.ImFix_cleaned = str_squish(Bef.ImFix_cleaned),
    Bef.ImFix_cleaned = tolower(Bef.ImFix_cleaned),
    Bef.ImFix_cleaned = gsub(stopwords.olli.col, "", Bef.ImFix_cleaned),
    Bef.ImFix_cleaned = str_squish(Bef.ImFix_cleaned)
   )

```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
#Bef.ImFix.freq.ngram <- 
EPcsv1 |> 
  count(Bef.ImFix_cleaned, sort = FALSE) |> 
  arrange(desc(n)) |>
  head(50) |>
  flextable() |> 
  width(j = 1, width = 6.5)
  
```

- Korrektur von Schreibfehlern
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
tipo.unauff <- EPcsv1 |> 
   na.omit() |>
   mutate(distance = stringdist("unauffällig", word1, method = "lv")) |>
   filter(distance <= 3)

```

## Erstellen neuer Daten {.justify}

### Auswertung der Befundtexte {.justify}
Als "ground truth" für die Auswertung der Chromatogramme dienen die Befundtexte der Immunfixationen. Diese Befundtext werden im folgenden ausgewertet.
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#Entfernen bestimmter Strings
# Laden des stringr-Pakets
library(stringr)

# Definieren der zu entfernenden Phrasen
phrasen <- c("ohne wesentliche Veränderung", "keine wesentliche Veränderung", "ohne signifikante Veränderung", "keine signifikante Veränderung")

# Erstellen eines regulären Ausdrucks, der alle Phrasen abdeckt
pattern <- paste(phrasen, collapse = "|")

# Funktion zum Entfernen der Phrasen und Bereinigen der Zeichenkette
entferne_phrasen <- function(text) {
  # Entfernen der definierten Phrasen
  text <- str_remove_all(text, pattern)
  return(text)
}

# Anwenden der Funktion auf den gesamten Vektor
EPcsv1$Bef.ImFix <- sapply(EPcsv1$Bef.ImFix, entferne_phrasen)


EPcsv1 <- EPcsv1 |> 
  mutate(
    suspicious = case_when(
      is.na(Bef.ImFix) ~ NA,  # Falls Bef.ImFix NA ist, wird suspicious NA
      grepl("M-Protein\\w|\\b(mono)?klonal(e|es|er|en)?\\b|Bande|Zonierung|Zonen|M-Protein|Para\\w|Inhomogen\\w|Unregelmässig\\w", Bef.ImFix, ignore.case = TRUE) ~ 1,
      TRUE ~ 0
    ),
    neg.phrase = case_when(
      is.na(Bef.ImFix) ~ NA,  # Falls Bef.ImFix NA ist, wird neg.phrase NA
      grepl("nicht|kein\\w|ohne|unauffällig", Bef.ImFix, ignore.case = TRUE) ~ 1,
      TRUE ~ 0
    ),
    EP.neg = case_when(
      is.na(Bef.EP) ~ NA,  # Falls Bef.EP NA ist, wird EP.neg NA
      grepl("unauffällig", Bef.EP, ignore.case = TRUE) ~ 1,
      TRUE ~ 0
    )
  )
```

### Weitere Auswertung der Befundtexte {.justify}
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# filtern EPcsv1$suspicious == 1 & EPcsv1$neg.phrase == 1
EPcsv.red <- EPcsv1 |> 
  filter(suspicious == 1 & neg.phrase == 1 & EP.neg == 0) |> 
  select(ID, Bef.EP, Bef.ImFix, suspicious, neg.phrase, EP.neg)


```

### peak detection im pathologischen Chromatogramm

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

library(pracma)



hex.string <- EPcsv1$curve[EPcsv1$ID == 1675]

# Split the string into hexadecimal parts
hex.parts <- substring(hex.string, seq(1, nchar(hex.string), 4), seq(4, nchar(hex.string), 4))

# Convert each hex part to decimal
decimal.values <- sapply(hex.parts, function(x) as.integer(paste0("0x", x)))

# dataframe with hexadecimal and decimal values
dat <- as.data.frame(decimal.values)
dat$decimal.values <- rev(dat$decimal.values)
dat$hex.parts <- hex.parts
dat$ID <- seq_len( nrow(dat) ) 

# delete rows with decimal values >10000
dat <- dat[dat$decimal.values < 10000,]

peaks <- findpeaks(dat$decimal.values, npeaks = 15, nups = 1, ndowns = 0, minpeakheight = 80)


plot(decimal.values ~ ID, data = dat, type = "l", col = "blue", lwd = 2, xlab = "ID", ylab = "signal", main = "EP Chromatogramm mit detektierten Peaks")
points(dat$ID[peaks[,2]], peaks[,1], col = "red", pch = 20)
abline(v = dat$ID[peaks[, 3]], col = "blue", lty = 2)  # Start of peaks
abline(v = dat$ID[peaks[, 4]], col = "blue", lty = 2)  # End of peaks
abline(v = dat$ID[160], col = "orange", lty = 2)
text(x = dat$ID[peaks[, 2]], y = peaks[, 1]+c(-500, 1, 200, 200, 1, 1 ), labels = c("Albumin", "alpha 1", "alpha 2", "beta 1", "beta 2", "gamma"),
     pos = c(2, 3, 3, 3, 3, 3), offset = 0.5  , col = "darkgreen")





``` 
# ```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# # calculate the area under the curve for all peaks
# 
# auc_all_peaks <- apply(peaks, 1, function(peak) {
#   start_index <- peak[3]  # Start index
#   end_index <- peak[4]    # End index
#   peak_time <- decimal.values$ID[start_index:end_index]
#   peak_signal <- decimal.values$decimal.values[start_index:end_index]
#   trapz(peak_time, peak_signal)  # Trapezoidal rule for AUC
# })
# 
# auc_all_peaks
#
#```

### peak detection im normalen Chromatogramm
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

library(pracma)



hex.string <- EPcsv1$curve[EPcsv1$ID == 1671]

# Split the string into hexadecimal parts
hex.parts <- substring(hex.string, seq(1, nchar(hex.string), 4), seq(4, nchar(hex.string), 4))

# Convert each hex part to decimal
decimal.values <- sapply(hex.parts, function(x) as.integer(paste0("0x", x)))

# dataframe with hexadecimal and decimal values
decimal.values <- as.data.frame(decimal.values)
decimal.values$hex.parts <- hex.parts
decimal.values$ID <- seq_len( nrow(decimal.values) ) |> rev()

# delete rows with decimal values >10000
decimal.values <- decimal.values[decimal.values$decimal.values < 10000,]

peaks <- findpeaks(decimal.values$decimal.values, npeaks = 15, nups = 1, ndowns = 0, minpeakheight = 80)


plot(decimal.values ~ ID, data = decimal.values, type = "l", col = "blue", lwd = 2, xlab = "ID", ylab = "signal", main = "normales EP Chromatogramm mit detektierten Peaks")
points(decimal.values$ID[peaks[,2]], peaks[,1], col = "red", pch = 20)
abline(v = decimal.values$ID[peaks[, 3]], col = "blue", lty = 2)  # Start of peaks
abline(v = decimal.values$ID[peaks[, 4]], col = "blue", lty = 2)  # End of peaks
``` 
### Area Under the Curve (AUC)

Die wichtigen Peaks für die Beurteilung einer Gammopathie sind die Peaks beta1, beta2, und gamma. Für diesen Bereicht des Chromatogramms werden im folgenden die Flächen zwischen den jeweils benachbarten Messpunkten berechnet.

```{r, message=FALSE, warning=FALSE}
# Filtern des Datensatzes für den gewünschten ID-Bereich
dat_filtered <- subset(dat, ID >= 160)

# Filtern der Peaks entsprechend dem gefilterten Datensatz
peaks_filtered <- peaks[peaks[, 2] >= 160, ]

# Erstellen des Plots mit dem gefilterten Datensatz
plot(decimal.values ~ ID, data = dat_filtered, type = "l", col = "blue", lwd = 2,
     xlab = "ID", ylab = "Signal", main = "EP Chromatogramm mit detektierten Peaks (ID 160-300)")

# Hinzufügen der Peak-Punkte
points(dat_filtered$ID[peaks_filtered[, 2] - 156], peaks_filtered[, 1], col = "red", pch = 20)

# Hinzufügen der vertikalen Linien für Peak-Start und -Ende
abline(v = dat_filtered$ID[peaks_filtered[, 3] - 156], col = "blue", lty = 2)  # Start der Peaks
abline(v = dat_filtered$ID[peaks_filtered[, 4] - 156], col = "blue", lty = 2)  # Ende der Peaks

# Hinzufügen der vertikalen Linie bei ID 160
abline(v = dat_filtered$ID[1], col = "orange", lty = 2)

# Hinzufügen der Beschriftungen mit spezifischen Offsets
text(x = dat_filtered$ID[peaks_filtered[, 2] - 159], 
     y = peaks_filtered[, 1],
     labels = c(#"Albumin", "alpha 1", "alpha 2", 
                "beta 1", 
                "beta 2", 
                "gamma"),
     pos = c(#2, 3, 3, 
             3, 3, 4), offset = 1, col = "darkgreen")

# Hinzufügen eines schattierten Bereichs zwischen ID 239 und 240
polygon(c(dat_filtered$ID[83:84], rev(dat_filtered$ID[83:84])),
        c(dat_filtered$decimal.values[83:84], rep(0, length(dat_filtered$decimal.values[83:84]))),
        col = "gray", border = NA)


```
Die grau eingefärbte Beispielfläche oben hat eine Fläche von:  


`r trapz(dat_filtered$ID[83:84], dat_filtered$decimal.values[83:84])`

Somit werden pro Chromatogramm 139 Flächen (AUCs) als features berechnet.

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# calculate the area under the curve for a section
dat$AUC <- rep(NA, nrow(dat))
for (i in 1:(nrow(dat))){
      dat$AUC[i] <- trapz(dat$ID[i:(i+1)], dat$decimal.values[i:(i+1)])
}
dat$AUC[is.na(dat$AUC)] <- 0
```


### Steigungen vor den Peaks
Adäquat wird zwischen diesen Punktepaaren auch die Steigung berechnet
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# calculate the slopes between the points 160 to 300
dat$slopes <- rep(0, nrow(dat))
for (i in 1:(nrow(dat))) {
      dat$slopes[i] <- (dat$decimal.values[i+1] - dat$decimal.values[i]) / ((dat$ID[i+1]) - (dat$ID[i]))
 }
dat$slopes[is.na(dat$slopes)] <- 0




```

### zweite Ableitung vor den Peaks 
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# calculate the second derivatives 

dat$der <- rep(NA, nrow(dat))
dat$der <- predict(smooth.spline(dat$ID, dat$decimal.values), deriv = 2)$y[1:(nrow(dat))]


```



## Integrieren von Daten {.justify}

## Formatieren von Daten {.justify}
