---
author: "Oliver Speer"
date: "10.12.2024"
---

::::: columns
::: {.column width="50%"}
:::

::: {.column width="50%"}
```{r version number, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
cat("Version:\n",format(file.info("DataPreparation.qmd")$mtime,"%d. %B %Y"))
```
:::
:::::

```{r, echo=FALSE, message=FALSE, warning=FALSE}
source("StartUp.R")
StartUpRoutine()
```

# Data Preparation {.unnumbered}


## Auswahl der Daten: Auswahl der relevanten Elemente und Merkmale {#var_selection .justify}




```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
library(dagitty)
dag <- dagitty('dag {
  age -> gammopathy
  food -> cholesterol
  cholesterol -> gammopathy
  smoking -> gammopathy
  alcohol -> liver_enzyme
  alcohol -> cholesterol
  gammopathy <- liver_enzyme
  sex -> gammopathy
  GUS -> gammopathy
  age -> GUS
  sex -> liver_enzyme
  sex -> smoking
  sex -> alcohol
  sex -> cholesterol
  sex -> GUS
  sex -> age
  age -> liver_enzyme
  age -> food
  age -> smoking
  age -> alcohol
  
}')

plot(dag)

adjustmentSets(dag, exposure = "sex", outcome = "gammopathy")
adjustmentSets(dag, exposure = "age", outcome = "gammopathy")
```


<!-- Sollte ich ein gam oder glm modellieren, dann kann ich aus dem oberen DAG mitnehmen, dass ich für "Alter" und "Geschlecht" adjustieren sollte. -->

<!-- [**Q: Aber wie wähle ich für ein random forest Modell die relevanten Variablen aus?**]{style="#00008B"} -->



## Bereinigen der Daten {.justify}
```{r read csv, echo=T, message=FALSE, warning=FALSE}
# Einlesen der Daten

EPcsv <- read_csv2("epcdatawithcurve.csv", col_select = c(1:4, 6), col_types = "icccc") |> 
  rename_with(~ c("ID", "TaNu", "Bef.EP", "Bef.ImFix", "curve"))
#str(EPcsv)
```
Datensätze ohne Chromatogramm-Daten, und auch "SISTIERTE" und "entfernte"  Datensätze werden entfernt.<br>
Kontrolle der Daten nach dem Filtern:

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# filtern der Daten EPcsv1$curve !NA
EPcsv1 <- EPcsv[!is.na(EPcsv$curve)&!is.na(EPcsv$Bef.ImFix),] |>   
  filter(!grepl("SISTIERT|entfernt", Bef.EP, ignore.case = TRUE)) |> 
  filter(!grepl("^(SISTIERT|entfernt|folgt)\\b", Bef.ImFix, ignore.case = TRUE)) |> 
  filter(!grepl("^X\\d", curve, ignore.case = FALSE))
 
```
- Von `r sum(is.na(EPcsv$curve))` Datensätzen ohne Chromatogramm-Daten  wurden `r sum(is.na(EPcsv$curve)) - sum(is.na(EPcsv1$curve))` Datensätze entfernt.

- Von `r sum(is.na(EPcsv$Bef.ImFix))` Datensätzen ohne iFix-Befundtexte wurden `r sum(is.na(EPcsv$Bef.ImFix)) - sum(is.na(EPcsv1$Bef.ImFix))` Datensätze entfernt.

- Von `r sum(EPcsv$Bef.ImFix == "SISTIERT", na.rm = T)` sistierten iFix-Aufträgen wurden `r sum(EPcsv$Bef.ImFix == "SISTIERT", na.rm = T) - sum(EPcsv1$Bef.ImFix == "SISTIERT", na.rm = T)`   iFix-Befunde entfernt. 

- Von `r sum(EPcsv$Bef.ImFix == "entfernt", na.rm = T)` entfernten iFix-Aufträgen wurden `r sum(EPcsv$Bef.ImFix == "entfernt", na.rm = T) - sum(EPcsv1$Bef.ImFix == "entfernt", na.rm = T)`   iFix-Befunde entfernt.   

- Von `r sum(grepl("^folgt\\b", EPcsv$Bef.ImFix, ignore.case = TRUE))`  iFix-Aufträgen, die auf "folgt" stehen, wurden `r sum(grepl("^folgt\\b", EPcsv$Bef.ImFix, ignore.case = TRUE)) - sum(grepl("^folgt\\b", EPcsv1$Bef.ImFix, ignore.case = TRUE))`   iFix-Befunde entfernt. 


::: {.panel-tabset}

### Bereinigung der Befundtexte
Wie im Kapitel "Data Understanding" beschrieben, werden die iFix-Befundtexte aufbereitet. Dazu werden Schreibfehler, Synonyme, Satzzeichen, Gross- und Kleinschreibung und Stopwörter bereinigt.
In den Tabsets werden die Stopwörterlisten angezeigt.  
- Entfernen der Klammern, Zahlen, Satzzeichen, Gross- und Kleinschreibung, Stopwörter und überflüssiger Leerzeichen.   
-Vereinheitlichung der Synonyme von "monoklonalen" und "M-Proteinen".

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Erstellen der Stopwörterliste
library(tidytext)
library(stringr)
library(stopwords)

# Monate als Stopwörter
monate <- c(
  "Januar", "Jan",
  "Februar", "Feb",
  "März", "März",
  "April", "Apr",
  "Mai", "Mai",
  "Juni", "Jun",
  "Juli", "Jul",
  "August", "Aug",
  "September", "Sept",
  "Oktober", "Okt",
  "November", "Nov",
  "Dezember", "Dez",
  "Monat", "Monaten"
) 

# Stopwörterliste aus den Befunden (manuell ergänzt)
ad.string <- c("ist die quantitative Bestimmung der aus der aktuellen Serumprobe und oder eine Verlaufskontrolle zu empfehlen Homogene Unspezifischer Befund vereinbar mit einer sowie der isolierten einer äusserst diskreten und diffusen in den Spuren für sicheren Je nach klinischem Kontext ist eine Verlaufskontrolle zu empfehlen und bedingt einen modifizierten Ansatz Das initial berichtete endogene Weiterhin schwacher des vorbekannten sowie der zusätzlichen diskreten in den Spuren wesentliche des stark ausgeprägten Mobilität sowie des sehr schwach ausgeprägten leicht einer äusserst diskreten und diffusen Befund mit einer schwach ausgeprägten diskret diffusen bekannten Bei unklarer Ätiologie Verlaufskontrolle diskreter Bei Ätiologie Verlaufskontrolle Typ Verlaufskontrolle B RGB initialen klin kb dv zusätzlich diskrete bekannte diskrete übrigen zeigt leichte passager klinik abklingen disret inhomogener transitorisch starken abklärung verdünnungsanalyse durchgeführt momentan extra peak beta fraktionen interferenz kontrastmittel medikamente handeln hinweis bitte markieren markieren analyse auftragsformular nicht gewünscht unserem prozess ep if automatisch gleichzeitig angesetzt danken verständnis zwischenbericht multipler ausgeprägter abklärung modifzierter durchgeführt vermutlich eher bedingter erwägen igg iga igm kappa lambda untan unten utnen pki osp maz kek")

# stopwords anpassen, da in den tokens "nicht", "kein", "ohne" und Varianten 
# enthalten bleiben sollen
stopwords.olli <- stopwords("de", "stopwords-iso")

stopwords.olli <- c(
                      stopwords.olli, ad.string |>
                      str_split(" ") |>
                      unlist(), monate
                    ) |>
  tolower() |>
  unique() |>
  setdiff("") |>
  sort()

stopwords.olli <- setdiff(stopwords.olli, 
                          c("nicht",
                            "nichts",
                            "keine", 
                            "kein", 
                            "keinem", 
                            "keinen", 
                            "keiner", 
                            "keines", 
                            "ohne",
                            "und?"))
```


### Stopwörterliste


```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
(stopwords.olli)
```



```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
stopwords.olli.col <- paste0("\\b(", paste(stopwords.olli, collapse = "|"), ")\\b")


```


### Bereinigte Befundtexte {.justify}


```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
EPcsv1 <- EPcsv1 |>
  mutate(
    Bef.ImFix_cleaned = gsub("\\([^)]*\\)", "", Bef.ImFix),
    Bef.ImFix_cleaned = gsub("[0-9]+", "", Bef.ImFix_cleaned),
    #Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, monate, ""),
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, "[[:punct:]]", " "),
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, " +", " "),
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, "°", ""),    
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, "\\b(mono)?klonale?n?s? Proteine?n?s?\\b", "MProtein"),
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, "\\bM Proteine?n?s?\\b", "MProtein"),
    Bef.ImFix_cleaned = str_squish(Bef.ImFix_cleaned),
    Bef.ImFix_cleaned = tolower(Bef.ImFix_cleaned),
    Bef.ImFix_cleaned = gsub(stopwords.olli.col, "", Bef.ImFix_cleaned),
    Bef.ImFix_cleaned = str_squish(Bef.ImFix_cleaned)
   )
```


```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: tbl-cleaned
#| tbl-cap: Datenüberblick nach der Bereinigung der Befundtexte

Bef.ImFix.freq <- EPcsv1 |> 
  count(Bef.ImFix_cleaned, sort = TRUE) |> 
  arrange(Bef.ImFix_cleaned)

ft1 <- Bef.ImFix.freq[1461:1490,] |> 
  arrange(desc(n)) |>
  #head(20) |> 
  flextable()

ft2 <- Bef.ImFix.freq[1611:1640,] |> 
  arrange(desc(n)) |>
  #head(20) |> 
  flextable()

ft3 <- Bef.ImFix.freq[1931:1960,] |> 
  arrange(desc(n)) |>
  #head(20) |> 
  flextable()

# HTML-Ausgabe erstellen
html_output <- htmltools::browsable(
  htmltools::tagList(
    htmltools::tags$div(
      style = "display: flex; justify-content: space-between;",
      htmltools::tags$div(
        flextable::htmltools_value(ft1),
        style = "margin-right: 10px;"
      ),
      htmltools::tags$div(
        flextable::htmltools_value(ft2),
        style = "margin-right: 10px;"
      ),      
      htmltools::tags$div(
        flextable::htmltools_value(ft3)
      )
    )
  )
)

# HTML-Ausgabe anzeigen
#print(html_output, browse = TRUE)
html_output

```

:::

Nach dem Bereiningen der Befundtexte werden die Rechtschreibfehler bestimmter Key-Wörter, die evtl zur Klassifizierung dienen, korrigiert. 

::: {.panel-tabset}

#### Korrektur von *"unauffällig"*
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Vektor mit uni-Gramen
unigram <- EPcsv1$Bef.ImFix_cleaned |> 
  str_split(" ") |>
  unlist() |> 
  unique() |> 
  as.data.frame() |> 
  setNames("unigram")

tipo.unauff <- unigram |> 
   na.omit() |>
   mutate(distance = stringdist("unauffällig", unigram, method = "lv")) |>
   filter(distance <= 3) |>
   pull(unigram)|>
   unique()


tipo.unauff <- paste0("\\b(", paste(tipo.unauff, collapse = "|"), ")\\b")

#paste0("\\b(", paste(stopwords.olli, collapse = "|"), ")\\b")

# alle Bef.ImFix_cleaned, die "unauffällig" ähnlich sind ersetzen
EPcsv1 <- EPcsv1 |>
  mutate(
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, tipo.unauff, "unauffällig")
  )

```

#### Kontrolle vor Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
tipo.unauff
```

#### Kontrolle nach Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
unigram <- EPcsv1$Bef.ImFix_cleaned |> 
  str_split(" ") |>
  unlist() |> 
  unique() |> 
  as.data.frame() |> 
  setNames("unigram")

unigram |> 
   na.omit() |>
   mutate(distance = stringdist("unauffällig", unigram, method = "lv")) |>
   filter(distance <= 3) |>
   pull(unigram)|>
   unique()

```

:::

::: {.panel-tabset}

#### Korrektur:  *"vorbefund"*
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}




tipo <- unigram |> 
   na.omit() |>
   mutate(distance = stringdist("vorbefund", unigram, method = "lv")) |>
   filter(distance <= 3) |>
   pull(unigram)|>
   unique()

tipo <- paste0("\\b(", paste(tipo, collapse = "|"), ")\\b")

#paste0("\\b(", paste(stopwords.olli, collapse = "|"), ")\\b")

# alle Bef.ImFix_cleaned, die "unauffällig" ähnlich sind ersetzen
EPcsv1 <- EPcsv1 |>
  mutate(
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, tipo, "vorbefund")
  )

```

#### Kontrolle vor Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
tipo
```

#### Kontrolle nach Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
unigram <- EPcsv1$Bef.ImFix_cleaned |> 
  str_split(" ") |>
  unlist() |> 
  unique() |> 
  as.data.frame() |> 
  setNames("unigram")

unigram |> 
   na.omit() |>
   mutate(distance = stringdist("vorbefund", unigram, method = "lv")) |>
   filter(distance <= 3) |>
   pull(unigram)|>
   unique()

```

:::

::: {.panel-tabset}

#### Korrektur: *"monoklonal"*
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}




tipo <- unigram |> 
   na.omit() |>
   mutate(distance = stringdist("monoklonal", unigram, method = "lv")) |>
  filter(!unigram %in% c("polyklonal", "poliklonal", "polyklonale", "oligoklonal", "poliklonale", "plyklonal")) |>
   filter(distance <= 4) |>
   pull(unigram)|>
   unique()

tipo <- paste0("\\b(", paste(tipo, collapse = "|"), ")\\b")

#paste0("\\b(", paste(stopwords.olli, collapse = "|"), ")\\b")

# alle Bef.ImFix_cleaned, die "monoklonal" ähnlich sind ersetzen
EPcsv1 <- EPcsv1 |>
  mutate(
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, tipo, "monoklonal")
  )

```

#### Kontrolle vor Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
tipo
```

#### Kontrolle nach Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
unigram <- EPcsv1$Bef.ImFix_cleaned |> 
  str_split(" ") |>
  unlist() |> 
  unique() |> 
  as.data.frame() |> 
  setNames("unigram")

unigram |> 
   na.omit() |>
   mutate(distance = stringdist("monoklonal", unigram, method = "lv")) |>
   filter(distance <= 4) |>
   pull(unigram)|>
   unique()

```

:::


::: {.panel-tabset}

#### Korrektur: *"polyklonal"*
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}




tipo <- unigram |> 
   na.omit() |>
   mutate(distance = stringdist("polyklonal", unigram, method = "lv")) |>
   filter(!unigram %in% c("monoklonal", "oligoklonal")) |>
   filter(distance <= 4) |>
   pull(unigram)|>
   unique()

tipo <- paste0("\\b(", paste(tipo, collapse = "|"), ")\\b")

#paste0("\\b(", paste(stopwords.olli, collapse = "|"), ")\\b")

# alle Bef.ImFix_cleaned, die "monoklonal" ähnlich sind ersetzen
EPcsv1 <- EPcsv1 |>
  mutate(
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, tipo, "polyklonal")
  )

```

#### Kontrolle vor Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
tipo
```

#### Kontrolle nach Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
unigram <- EPcsv1$Bef.ImFix_cleaned |> 
  str_split(" ") |>
  unlist() |> 
  unique() |> 
  as.data.frame() |> 
  setNames("unigram")

unigram |> 
   na.omit() |>
   mutate(distance = stringdist("polyklonal", unigram, method = "lv")) |>
   filter(distance <= 4) |>
   pull(unigram)|>
   unique()

```

:::


::: {.panel-tabset}

#### Korrektur *"veränderung"*
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

wort <- "veränderung"
dist <- 2


tipo <- unigram |> 
   na.omit() |>
   mutate(distance = stringdist(wort, unigram, method = "lv")) |>
   filter(!unigram %in% c("monoklonal", "oligoklonal")) |>
   filter(distance <= dist) |>
   pull(unigram)|>
   unique()

tipo <- paste0("\\b(", paste(tipo, collapse = "|"), ")\\b")

#paste0("\\b(", paste(stopwords.olli, collapse = "|"), ")\\b")

# alle Bef.ImFix_cleaned, die "wort" ähnlich sind ersetzen
EPcsv1 <- EPcsv1 |>
  mutate(
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, tipo, wort)
  )

```

#### Kontrolle vor Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
tipo
```

#### Kontrolle nach Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
unigram <- EPcsv1$Bef.ImFix_cleaned |> 
  str_split(" ") |>
  unlist() |> 
  unique() |> 
  as.data.frame() |> 
  setNames("unigram")

unigram |> 
   na.omit() |>
   mutate(distance = stringdist(wort, unigram, method = "lv")) |>
   filter(distance <= dist) |>
   pull(unigram)|>
   unique()

```

:::


::: {.panel-tabset}

#### Korrektur *"nachweis"*
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

wort <- "nachweis"
dist <- 3


tipo <- unigram |> 
   na.omit() |>
   mutate(distance = stringdist(wort, unigram, method = "lv")) |>
   filter(!unigram %in% c("monoklonal", "oligoklonal")) |>
   filter(distance <= dist) |>
   pull(unigram)|>
   unique()

tipo <- paste0("\\b(", paste(tipo, collapse = "|"), ")\\b")

#paste0("\\b(", paste(stopwords.olli, collapse = "|"), ")\\b")

# alle Bef.ImFix_cleaned, die "monoklonal" ähnlich sind ersetzen
EPcsv1 <- EPcsv1 |>
  mutate(
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, tipo, wort)
  )

```

#### Kontrolle vor Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
tipo
```

#### Kontrolle nach Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
unigram <- EPcsv1$Bef.ImFix_cleaned |> 
  str_split(" ") |>
  unlist() |> 
  unique() |> 
  as.data.frame() |> 
  setNames("unigram")

unigram |> 
   na.omit() |>
   mutate(distance = stringdist(wort, unigram, method = "lv")) |>
   filter(distance <= dist) |>
   pull(unigram)|>
   unique()

```

:::

::: {.panel-tabset}

#### Korrektur *"immunreaktives"*
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

wort <- "immunreaktives"
dist <- 5


tipo <- unigram |> 
   na.omit() |>
   mutate(distance = stringdist(wort, unigram, method = "lv")) |>
   filter(!unigram %in% c("monoklonal", "oligoklonal")) |>
   filter(distance <= dist) |>
   pull(unigram)|>
   unique()

tipo <- paste0("\\b(", paste(tipo, collapse = "|"), ")\\b")

#paste0("\\b(", paste(stopwords.olli, collapse = "|"), ")\\b")

# alle Bef.ImFix_cleaned, die "monoklonal" ähnlich sind ersetzen
EPcsv1 <- EPcsv1 |>
  mutate(
    Bef.ImFix_cleaned = str_replace_all(Bef.ImFix_cleaned, tipo, wort)
  )

```

#### Kontrolle vor Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
tipo
```

#### Kontrolle nach Korrektur
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}



unigram <- EPcsv1$Bef.ImFix_cleaned |> 
  str_split(" ") |>
  unlist() |> 
  unique() |> 
  as.data.frame() |> 
  setNames("unigram")

unigram |> 
   na.omit() |>
   mutate(distance = stringdist(wort, unigram, method = "lv")) |>
   filter(distance <= dist) |>
   pull(unigram)|>
   unique()

```

:::

::: {.panel-tabset}

Nach Bereinigung und Korrektur können die Befundtext tokenisiert werden.  

### Tokenisierung {.justify}
Beim tokenisieren wird der Befundtext in einzelne Wörter zerlegt, alles auf Kleinbuchstaben umgewandelt und die Wörter als Tri-Grame und Uni-Gramme abgespeichert. Die Trigrame geben den Kontext der Wörter wieder und können als Features für die Klassifizierung verwendet werden.<br>
Für das kommende Clustering wären Tri-Gramme zwar von Interesse, jedoch hat es von den jeweiligen Trigrammen nur realtiv wenige. Daraus würde eine "sparse matrix" entstehen. Daher werden die Uni-Gramme für das Clustering verwendet.


```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Zeilen mit Bef.ImFix_cleaned = "" entfernen
EPcsv1 <- EPcsv1 |> 
  filter(Bef.ImFix_cleaned != "")

EPcsv3 <- EPcsv1|> 
  unnest_tokens(trigram, Bef.ImFix_cleaned, token = "ngrams", n = 3, to_lower =TRUE, drop = FALSE)

EPcsv.uni <- EPcsv1|> 
  unnest_tokens(trigram, Bef.ImFix_cleaned, to_lower =TRUE, drop = FALSE)



```

### Häufigste Unigramme {.justify}
```{r, echo=TRUE, message=FALSE, warning=FALSE}
#| label: tbl-trigram
#| tbl-cap: Häufigste Unigramme in den Befundtexten

EPcsv.uni <- readRDS("EPcsv.uni.rds")
  
#Bef.ImFix.freq.ngram <- 
EPcsv.uni |> 
  count(trigram, sort = FALSE) |> 
  arrange(desc(n)) |>
  head(50) |>
  flextable() |> 
  width(j = 1, width = 3.5)
  
```



:::



## Erstellen neuer Daten {.justify}

### Clustering und Klassifizierung der Befundtexte {.justify}
Als "ground truth" für die Auswertung der Chromatogramme dienen die Befundtexte der Immunfixationen. Diese Befundtext werden im folgenden geclustert und klassifiziert.

#### Unsupervised Clustering mit K-Means {.justify}




<!-- Zuerst werden die strings "unauffällig" von EP.csv3$Bef.ImFix_cleaned nach trigrams kopiert -->
<!-- ```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- # erkennen der Einträge mit "unauffällig" und einschreiben von "unauffällig" in trigram ein -->
<!-- EPcsv3$trigram <- ifelse(grepl("^unauffällig$", EPcsv3$Bef.ImFix_cleaned), "unauffällig", EPcsv3$trigram) -->
<!-- EPcsv3$trigram <- ifelse(grepl("^unauffällig hypogammaglobulinämie$", EPcsv3$Bef.ImFix_cleaned), "unauffällig hypogammaglobulinämie", EPcsv3$trigram) -->
<!-- ``` -->

##### Tri-Gramme extrahieren und clustern
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Tri-Gramme extrahieren
tri.grams <- EPcsv.uni |> 
  select(ID, trigram) |>
  count(ID, trigram, sort = TRUE) |> 
  arrange(ID) 

# Tri-Gramme in Matrix umwandeln
tri.grams.matrix <- tri.grams |> 
  pivot_wider(names_from = trigram, values_from = n, values_fill = 0) |> 
  select(-ID) |> 
  as.matrix()
```

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
library(furrr)
library(future)

plan(multisession, workers = 10)

set.seed(8173)
wss <- future_map_dbl(
  1:10, 
  ~ kmeans(tri.grams.matrix, centers = ., nstart = 25)$tot.withinss,
  .options = furrr_options(seed = TRUE)
)

plan(sequential)

saveRDS(wss, "wss.rds")
```


```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-cluster
#| fig-cap: Elbow-Plot

wss <- readRDS("wss.rds")

# Elbow-Plot
plot(1:10, wss, type = "b", pch = 19, frame = FALSE,
     xlab = "Anzahl der Cluster",
     ylab = "Within-Cluster Sum of Squares (WSS)",
     main = "")
```


```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}


library(furrr)
library(future)
library(tibble)

plan(multisession, workers = 4)

set.seed(8173)

# verschiedene Werte für nstart
nstart_values <- c(1, 5, 10, 25 #, 50, 100
                   )

# K-Means mit verschiedenen nstart-Werten parallelisiert
results.nstart <- future_map_dfr(nstart_values, function(nstart) {
  model <- kmeans(tri.grams.matrix, centers = 4, nstart = nstart)
  tibble(nstart = nstart, tot_withinss = model$tot.withinss)
}, .options = furrr_options(seed = TRUE))

# results <- map_dfr(nstart_values, function(nstart) {
#   model <- kmeans(tri.grams.matrix, centers = 4, nstart = nstart)
#   tibble(nstart = nstart, tot_withinss = model$tot.withinss)
# })

plan(sequential)

saveRDS(results.nstart, "results_nstart.rds")

```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-nstart
#| fig-cap: Optimierung von nstart

results.nstart <- readRDS("results_nstart.rds")
# Plot der Ergebnisse
ggplot(results.nstart, aes(x = nstart, y = tot_withinss)) +
  geom_line() +
  geom_point() +
  labs(title = "",
       x = "nstart",
       y = "Totale Within-Cluster Sum of Squares") +
  theme_minimal()
```


```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# K-Means Clustering
set.seed(8173)
kmeans_model.c3 <- kmeans(tri.grams.matrix, centers = 3, nstart = 25)
kmeans_model.c4 <- kmeans(tri.grams.matrix, centers = 4, nstart = 25)
kmeans_model.c5 <- kmeans(tri.grams.matrix, centers = 5, nstart = 25)
#kmeans_model.c6 <- kmeans(tri.grams.matrix, centers = 6, nstart = 25)





# Cluster zuordnen
EPcsv1 <- EPcsv1 |> 
  mutate(cluster3 = kmeans_model.c3$cluster#,
         cluster4 = kmeans_model.c4$cluster,
         cluster5 = kmeans_model.c5$cluster
         )
```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-3cluster in wordcloud
#| fig-cap: 3 Cluster - Wortwolken
#| 
EPcsv1 <- readRDS("dev.df1.rds")

library(wordcloud)
cluster3.ngrams <- EPcsv1 |> 
  select(ID, cluster3, Bef.ImFix_cleaned) |>
  mutate(cluster3 = as.factor(cluster3)) |>
  count(cluster3, Bef.ImFix_cleaned, name = "n", sort = TRUE) |>
  group_by(cluster3, Bef.ImFix_cleaned)  |> 
  summarise(frequency = sum(n), .groups = "drop") 


  

# Wortwolke pro Cluster
#par(mfrow=c(2,2))
set.seed(5173)
cluster3.ngrams |> 
  filter(cluster3 == 1)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 20, colors = brewer.pal(8, "Dark2")))

cluster3.ngrams |> 
  filter(cluster3 == 2)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 60, colors = brewer.pal(8, "Dark2")))

cluster3.ngrams |> 
  filter(cluster3 == 3)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 60, colors = brewer.pal(8, "Dark2")))


```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
cluster3.ngrams |> 
  group_by(cluster3) |> 
  slice_max(order_by = frequency, n = 10) |> 
  arrange(cluster3, frequency) |> # Sortiere nach Cluster und Häufigkeit
  mutate(Bef.ImFix_cleaned = factor(Bef.ImFix_cleaned, levels = unique(Bef.ImFix_cleaned))) |> 
  ggplot(aes(x = Bef.ImFix_cleaned, y = frequency, fill = factor(cluster3))) +
  geom_bar(stat = "identity") +
  scale_y_log10() +
  coord_flip() +
  labs(title = "Häufigste Tri-Gramme pro Cluster",
       x = "texte",
       y = "Häufigkeit") +
  theme_minimal()

```


```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-4cluster in wordcloud
#| fig-cap: 4 Cluster - Wortwolken

library(wordcloud)
cluster4.ngrams <- EPcsv1 |> 
  select(ID, cluster4, Bef.ImFix_cleaned) |>
  count(cluster4, Bef.ImFix_cleaned, name = "n", sort = TRUE) |>
  group_by(cluster4, Bef.ImFix_cleaned)  |> 
  summarise(frequency = sum(n), .groups = "drop")

# Wortwolke pro Cluster
#par(mfrow=c(2,2))
set.seed(8173)
cluster4.ngrams |> 
  filter(cluster4 == 1)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 5, colors = brewer.pal(8, "Dark2")))

cluster4.ngrams |> 
  filter(cluster4 == 2)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 60, colors = brewer.pal(8, "Dark2")))

cluster4.ngrams |> 
  filter(cluster4 == 3)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 60, colors = brewer.pal(8, "Dark2")))

cluster4.ngrams |> 
  filter(cluster4 == 4)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 60, colors = brewer.pal(8, "Dark2")))
```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
cluster4.ngrams |> 
  group_by(cluster4) |> 
  slice_max(order_by = frequency, n = 10) |> 
  arrange(cluster4, desc(frequency)) |> # Sortiere nach Cluster und Häufigkeit
  mutate(Bef.ImFix_cleaned = factor(Bef.ImFix_cleaned, levels = unique(Bef.ImFix_cleaned))) |> 
  ggplot(aes(x = Bef.ImFix_cleaned, y = frequency, fill = factor(cluster4))) +
  geom_bar(stat = "identity") +
  scale_y_log10() +
  coord_flip() +
  labs(title = "Häufigste Tri-Gramme pro Cluster",
       x = "texte",
       y = "Häufigkeit") +
  theme_minimal()

```
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-5cluster in wordcloud
#| fig-cap: 5 Cluster - Wortwolken

library(wordcloud)
cluster5.ngrams <- EPcsv1 |> 
  select(ID, cluster5, Bef.ImFix_cleaned) |>
  count(cluster5, Bef.ImFix_cleaned, name = "n", sort = TRUE) |>
  group_by(cluster5, Bef.ImFix_cleaned)  |> 
  summarise(frequency = sum(n), .groups = "drop")

# Wortwolke pro Cluster
#par(mfrow=c(2,2))
set.seed(173)
cluster5.ngrams |> 
  filter(cluster5 == 1)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 30, colors = brewer.pal(8, "Dark2")))

cluster5.ngrams |> 
  filter(cluster5 == 2)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 5, colors = brewer.pal(8, "Dark2")))

cluster5.ngrams |> 
  filter(cluster5 == 3)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 60, colors = brewer.pal(8, "Dark2")))

cluster5.ngrams |> 
  filter(cluster5 == 4)  |> 
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 60, colors = brewer.pal(8, "Dark2")))

cluster5.ngrams |> 
  filter(cluster5 == 5)  |> 
  arrange(desc(frequency)) |>
  with(wordcloud(Bef.ImFix_cleaned, frequency, max.words = 5, colors = brewer.pal(8, "Dark2")))
```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
cluster5.ngrams |> 
  group_by(cluster5) |> 
  slice_max(order_by = frequency, n = 4) |> 
  arrange(cluster5, frequency) |> 
  mutate(Bef.ImFix_cleaned = factor(Bef.ImFix_cleaned, levels = unique(Bef.ImFix_cleaned))) |> 
  ggplot(aes(x = Bef.ImFix_cleaned, y = frequency, fill = factor(cluster5))) +
  geom_bar(stat = "identity") +
  scale_y_log10() +
  coord_flip() +
  labs(title = "Häufigste Tri-Gramme pro Cluster",
       x = "texte",
       y = "Häufigkeit") +
  theme_minimal()


# Funktion, um Text auf maximal 10 Wörter zu kürzen
shorten.text <- function(text, max.words = 7) {
  words <- unlist(strsplit(text, "\\s+")) # Text in Wörter aufteilen
  if (length(words) > max.words) {
    paste0(paste(words[1:max.words], collapse = " "), "...")
  } else {
    text
  }
}

cluster5.ngrams |> 
  group_by(cluster5) |> 
  slice_max(order_by = frequency, n = 8) |> 
  mutate(
    Bef.ImFix_cleaned.short = sapply(Bef.ImFix_cleaned, shorten.text), # Texte kürzen
    Bef.ImFix_cleaned.short = factor(Bef.ImFix_cleaned.short, levels = unique(Bef.ImFix_cleaned.short))
  ) |> 
  ggplot(aes(x = Bef.ImFix_cleaned.short, y = frequency, fill = factor(cluster5))) +
  geom_bar(stat = "identity") +
  scale_y_log10() +
  coord_flip() +
  labs(title = "Häufigste Texte pro Cluster",
       x = "texte",
       y = "Häufigkeit") +
  theme_minimal()

```

#### Clusterzuordnung der Befundtexte als Klassifizierung

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Cluster-Zuordnung
EPcsv1  <-  EPcsv1 |> 
  mutate(
    Klassifizierung = case_when(
      cluster3 == 1 ~ "MGradient",
      cluster3 == 2 ~ "unauffällig",
      cluster3 == 3 ~ "suspicous"
    )|> 
  as.factor()
  ) 

# Speichern der Cluster und Klassifizierung
saveRDS(EPcsv1, "EPcsv1.rds")
saveRDS(EPcsv3, "EPcsv3.rds")
saveRDS(EPcsv.uni, "EPcsv.uni.rds")
```





```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-class_distribution
#| fig-cap: Verteilung der Klassifizierungen

EPcsv1 <- readRDS("~/CAS-EP&ML/dev_df1.rds")
# Anzahl der 3 verschiedenen Klassifizierungen als Barplot und Tabelle
EPcsv1 |> 
  ggplot(aes(x = Klassifizierung, fill = factor(cluster3))) +
  geom_bar() +
  labs(title = "Klassifizierung der Befundtexte",
       x = "Klassifizierung",
       y = "Anzahl") +
  coord_flip() +
  theme_minimal()
```


```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: tbl-class_distribution
#| tbl-cap: Verteilung der Klassifizierungen
ft <- EPcsv1 |> 
  count(Klassifizierung) |> 
  flextable() |>
  width(j = 1, width = 3.5)
ft

```
  
```{r, echo=FALSE, eval=FALSE, message=FALSE, warning=FALSE}
# 24-01-10 neue cluster Zuordnung
EPcsv.km.unauf  <-  EPcsv1 |> 
  select(ID, Bef.EP, Bef.ImFix, Bef.ImFix_cleaned, cluster3) |>
  mutate(
    Klassifizierung = case_when(
      grepl("unauffällig", Bef.ImFix_cleaned, ignore.case = T) ~ "unauffällig",
      cluster3 == 1 ~ "MGradient",
      #cluster3 == 2 ~ "unauffällig",
      cluster3 == 3 ~ "suspicous"
    )|> 
  as.factor()
  )

ft <- EPcsv.km.unauf |> 
  count(Klassifizierung) |> 
  flextable() |>
  width(j = 1, width = 3.5)
ft

# dieser Ansatz verbessert die Zuordnung der tatsächlich unauffälligen Befunde, 
#aber es bringt die Gruppe nicht eindeutig formulierter Befunde mit "NA" hervor, 
# zBsp: "keine Veränderung zum Vorbefund", "Bande in Schwere und leichte Kette"
```


### Pathologisches Chromatogramm

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-peak-patho

library(pracma)



hex.string <- EPcsv1$curve[EPcsv1$ID == 1675]

# Split the string into hexadecimal parts
hex.parts <- substring(hex.string, seq(1, nchar(hex.string), 4), seq(4, nchar(hex.string), 4))

# Convert each hex part to decimal
decimal.values <- sapply(hex.parts, function(x) as.integer(paste0("0x", x)))

# dataframe with hexadecimal and decimal values
dat <- as.data.frame(decimal.values)
dat$decimal.values <- rev(dat$decimal.values)
dat$hex.parts <- hex.parts
dat$ID <- seq_len( nrow(dat) ) 

# delete rows with decimal values >10000
dat <- dat[dat$decimal.values < 10000,]

peaks <- findpeaks(dat$decimal.values, npeaks = 15, nups = 1, ndowns = 0, minpeakheight = 80)


plot(decimal.values ~ ID, data = dat, type = "l", col = "blue", lwd = 2, xlab = "ID", ylab = "signal", main = "EP Chromatogramm mit detektierten Peaks")
points(dat$ID[peaks[,2]], peaks[,1], col = "red", pch = 20)
abline(v = dat$ID[peaks[, 3]], col = "blue", lty = 2)  # Start of peaks
abline(v = dat$ID[peaks[, 4]], col = "blue", lty = 2)  # End of peaks
abline(v = dat$ID[160], col = "orange", lty = 2)
text(x = dat$ID[peaks[, 2]], y = peaks[, 1]+c(-500, 1, 200, 200, 1, 1 ), labels = c("Albumin", "alpha 1", "alpha 2", "beta 1", "beta 2", "gamma"),
     pos = c(2, 3, 3, 3, 3, 3), offset = 0.5  , col = "darkgreen")





``` 

Im folgenden werden die Steigungen m, zweite Ableitungen fdd und die Flächen AUC bei allen Datenpunkten der Chromatogramme berechnet.

<!-- # ```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- # # calculate the area under the curve for all peaks -->
<!-- #  -->
<!-- # auc_all_peaks <- apply(peaks, 1, function(peak) { -->
<!-- #   start_index <- peak[3]  # Start index -->
<!-- #   end_index <- peak[4]    # End index -->
<!-- #   peak_time <- decimal.values$ID[start_index:end_index] -->
<!-- #   peak_signal <- decimal.values$decimal.values[start_index:end_index] -->
<!-- #   trapz(peak_time, peak_signal)  # Trapezoidal rule for AUC -->
<!-- # }) -->
<!-- #  -->
<!-- # auc_all_peaks -->
<!-- # -->
<!-- #``` -->

<!-- ### Normales Chromatogramm -->
<!-- ```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->

<!-- library(pracma) -->



<!-- hex.string <- EPcsv1$curve[EPcsv1$ID == 2705] -->

<!-- # Split the string into hexadecimal parts -->
<!-- hex.parts <- substring(hex.string, seq(1, nchar(hex.string), 4), seq(4, nchar(hex.string), 4)) -->

<!-- # Convert each hex part to decimal -->
<!-- decimal.values <- sapply(hex.parts, function(x) as.integer(paste0("0x", x))) -->

<!-- # dataframe with hexadecimal and decimal values -->
<!-- dat <- as.data.frame(decimal.values) -->
<!-- dat$decimal.values <- rev(dat$decimal.values) -->
<!-- dat$hex.parts <- hex.parts -->
<!-- dat$ID <- seq_len( nrow(dat) )  -->

<!-- # delete rows with decimal values >10000 -->
<!-- dat <- dat[dat$decimal.values < 10000,] -->

<!-- peaks <- findpeaks(dat$decimal.values, npeaks = 15, nups = 2, ndowns = 0, minpeakheight = 80) -->


<!-- plot(decimal.values ~ ID, data = dat, type = "l", col = "blue", lwd = 2, xlab = "ID", ylab = "signal", main = "EP Chromatogramm mit detektierten Peaks") -->
<!-- points(dat$ID[peaks[,2]], peaks[,1], col = "red", pch = 20) -->
<!-- abline(v = dat$ID[peaks[, 3]], col = "blue", lty = 2)  # Start of peaks -->
<!-- abline(v = dat$ID[peaks[, 4]], col = "blue", lty = 2)  # End of peaks -->
<!-- abline(v = dat$ID[160], col = "orange", lty = 2) -->
<!-- text(x = dat$ID[peaks[, 2]], y = peaks[, 1]+c(-500, 1, 200, 200, 1, 1 ), labels = c("Albumin", "alpha 1", "alpha 2", "beta 1", "beta 2", "gamma"), -->
<!--      pos = c(2, 3, 3, 3, 3, 3), offset = 0.5  , col = "darkgreen") -->

<!-- ```  -->



<!-- ```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- # kopieren der ersten 50 Zeilen von EPcsv1 in dev.df -->
<!-- dev.df <- EPcsv1 -->

<!-- # Hex-String in decimale Y-Werte umwandeln -->
<!-- convert.curve.to.y <- function(hex.string) { -->
<!--   hex.parts <- substring(hex.string, seq(1, nchar(hex.string), 4), seq(4, nchar(hex.string), 4)) -->
<!--   decimal.values <- sapply(hex.parts, function(x) as.integer(paste0("0x", x))) -->
<!--   dat <- as.data.frame(decimal.values) -->
<!--   dat$decimal.values <- rev(dat$decimal.values) -->
<!--   dat$ID <- seq_len( nrow(dat) )  -->
<!--   dat <- dat[dat$decimal.values < 10000,] -->
<!--   return(dat) -->
<!-- } -->

<!-- # Hinzufügen der 300 Y-Werte als Liste -->
<!-- dev.df <- dev.df |> -->
<!--   rowwise() |> -->
<!--   mutate(y.values = list(convert.curve.to.y(curve))) -->


<!-- # Funktion zur Peak-Erkennung und Umwandlung in ein DataFrame -->
<!-- library(pracma) -->

<!-- detect.peaks <- function(y.values) { -->
<!--   peaks <- findpeaks(y.values, npeaks = 15, nups = 2, ndowns = 0, minpeakheight = 80)   -->
<!--   if (is.null(peaks)) { -->
<!--     return(data.frame( -->
<!--       height = NA, position = NA, begin = NA, end = NA -->
<!--     )) -->
<!--   } -->
<!--   return(data.frame( -->
<!--     height = peaks[, 1], -->
<!--     position = peaks[, 2], -->
<!--     begin = peaks[, 3], -->
<!--     end = peaks[, 4] -->
<!--   )) -->
<!-- } -->

<!-- # Peaks finden und Ergebnisse als Liste hinzufügen -->
<!-- dev.df <- dev.df |> -->
<!--   rowwise() |> -->
<!--   mutate(peaks = list(detect.peaks(y.values$decimal.values))) -->

<!-- # Peaks in Wide-Format transformieren -->
<!-- extract.peak.data <- function(peaks, max.peaks = 12) { -->
<!--   peaks <- peaks[seq_len(min(nrow(peaks), max.peaks)), ] -->
<!--   result <- as.list(rep(NA, max.peaks * 4)) -->
<!--   names(result) <- paste0(rep(paste0("peak", seq_len(max.peaks)), each = 4), c(".height", ".position", ".begin", ".end")) -->
<!--   for (i in seq_len(nrow(peaks))) { -->
<!--     result[[paste0("peak", i, ".height")]] <- peaks$height[i] -->
<!--     result[[paste0("peak", i, ".position")]] <- peaks$position[i] -->
<!--     result[[paste0("peak", i, ".begin")]] <- peaks$begin[i] -->
<!--     result[[paste0("peak", i, ".end")]] <- peaks$end[i] -->
<!--   } -->
<!--   return(result) -->
<!-- } -->

<!-- # Ergebnisse als Spalten hinzufügen -->
<!-- dev.df <- dev.df |> -->
<!--   rowwise() |> -->
<!--   mutate(peak.data = list(extract.peak.data(peaks))) |> -->
<!--   unnest_wider(peak.data) -->
<!-- ``` -->






### Area Under the Curve (AUC)

Die wichtigen Peaks für die Beurteilung einer Gammopathie sind die Peaks beta1, beta2, und gamma. Für diesen Bereicht des Chromatogramms werden im folgenden die Flächen zwischen den jeweils 3 benachbarten Messpunkten nach Simpson berechnet.

```{r, message=FALSE, warning=FALSE}
#| label: fig-betagamma
#| fig-cap: Darstellung des kathodischen Teils mit den Peaks beta1, beta2 und gamma

source("fun.prep.EP.data.R")

dat <- fun.prep.EP.data(1675)

# Entfernen von Werten > 10000
dat <- dat[dat$decimal.values < 10000,]

# Filtern des Datensatzes für den gewünschten ID-Bereich
dat_filtered <- subset(dat, ID >= 160)

# Filtern der Peaks entsprechend dem gefilterten Datensatz
peaks_filtered <- peaks[peaks[, 2] >= 160, ]

# Erstellen des Plots mit dem gefilterten Datensatz
plot(decimal.values ~ ID, data = dat_filtered, type = "l", col = "blue", lwd = 2,
     xlab = "ID", ylab = "Signal", main = "EP Chromatogramm mit detektierten Peaks (ID 160-300)")

# Hinzufügen der Peak-Punkte
points(dat_filtered$ID[peaks_filtered[, 2] - 156], peaks_filtered[, 1], col = "red", pch = 20)

# Hinzufügen der vertikalen Linien für Peak-Start und -Ende
abline(v = dat_filtered$ID[peaks_filtered[, 3] - 156], col = "blue", lty = 2)  # Start der Peaks
abline(v = dat_filtered$ID[peaks_filtered[, 4] - 156], col = "blue", lty = 2)  # Ende der Peaks

# Hinzufügen der vertikalen Linie bei ID 160
abline(v = dat_filtered$ID[1], col = "orange", lty = 2)

# Hinzufügen der Beschriftungen mit spezifischen Offsets
text(x = dat_filtered$ID[peaks_filtered[, 2] - 159], 
     y = peaks_filtered[, 1],
     labels = c(#"Albumin", "alpha 1", "alpha 2", 
                "beta 1", 
                "beta 2", 
                "gamma"),
     pos = c(#2, 3, 3, 
             3, 3, 4), offset = 1, col = "darkgreen")

# Hinzufügen eines schattierten Bereichs zwischen ID 239 und 240
polygon(c(dat_filtered$ID[90:92], rev(dat_filtered$ID[90:92])),
        c(dat_filtered$decimal.values[90:92], rep(0, length(dat_filtered$decimal.values[90:92]))),
        col = "gray", border = NA)


```
Die grau eingefärbte Beispielfläche oben hat eine Fläche von:  


`r trapz(dat_filtered$ID[90:92], dat_filtered$decimal.values[90:92])`

Somit werden pro Chromatogramm 300 Flächen (AUCs) als features berechnet.

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# calculate the area under the curve for a section
dat$AUC <- rep(NA, nrow(dat))
for (i in 1:(nrow(dat))){
      dat$AUC[i] <- trapz(dat$ID[i:(i+1)], dat$decimal.values[i:(i+1)])
}
dat$AUC[is.na(dat$AUC)] <- 0
```


### Steigungen vor den Peaks
Adäquat wird zwischen diesen Punktepaaren auch die Steigung berechnet
```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# calculate the slopes between the points 160 to 300
dat$slopes <- rep(0, nrow(dat))
for (i in 1:(nrow(dat))) {
      dat$slopes[i] <- (dat$decimal.values[i+1] - dat$decimal.values[i]) / ((dat$ID[i+1]) - (dat$ID[i]))
 }
dat$slopes[is.na(dat$slopes)] <- 0




```

### zweite Ableitung vor den Peaks 
```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# calculate the second derivatives 

dat$der <- rep(NA, nrow(dat))
dat$der <- predict(smooth.spline(dat$ID, dat$decimal.values), deriv = 2)$y[1:(nrow(dat))]


```

Da die Anzahl der Peaks in den Chromatogrammen variieren kann, verfolge ich das "Peakdetection" nicht weiter.
Als features werden die Höhen, Flächen, die Steigungen und die zweiten Ableitungen für alle Messpunkte berechnet.
Bei der Modell-Entwicklung können dann die relevanten Bereiche der Chromatogramme ausgewählt werden.
Dies erlaubt dann evtl weiter Möglichkeiten über das vorliegende Projekt hinaus. 


## Integrieren und formatieren von Daten {.justify}

::: {.panel-tabset}

### Generieren der Steigungen m, Ableitungen fdd, und Flächen AUC und Speichern der Daten {.justify}


```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
library(tibble)
library(dplyr)
library(purrr)
library(future)
library(future.apply)
library(parallelly)

dev.df1 <- EPcsv1|> #[1:500,] 
   filter(!grepl("^X\\d", curve, ignore.case = FALSE)) # Entfernen von Zeilen, die mit "X" beginnen
  
# Initialisieren der neuen Spalten in EPcsv1
num_points <- 300 # Anzahl von EP.y-Werten
# Erzeugen der neuen Spalten explizit
for (col_name in c("EP.y", "m", "fdd", "AUC")) {
  for (i in seq_len(num_points)) {
    dev.df1[[paste0(col_name, i)]] <- NA_real_
  }
}

#fun.proc.ep <- function(dev.df1, hex_column, num_points){
  
# Prozessieren jedes Chromatogramms
 for (i in seq_len(nrow(dev.df1))) {
  #hex.string <- dev.df1[[hex_column]][i]
  hex.string <- dev.df1$curve[i]
  
  # Konvertieren der Hex-Strings in Dezimalwerte
  EP.dec.y <- as.integer(paste0("0x", substring(hex.string, seq(1, nchar(hex.string), 4), seq(4, nchar(hex.string), 4))))
  
  # Ersetzen der Werte > 10000 durch den Wert der vorhergehenden Stelle
  EP.dec.y <- cumsum(ifelse(EP.dec.y > 10000, 0, EP.dec.y)) - c(0, head(cumsum(ifelse(EP.dec.y > 10000, 0, EP.dec.y)), -1))
  

  # Erstellen des DataFrames mit EP elution time points, decimal EP signal values, slopes, second derivatives, und AUC 
  dat <- tibble(EP.t = seq_len( length(EP.dec.y) ), EP.y = rev(EP.dec.y))
  dat <- dat |>
    mutate(m = ((lead(EP.y) - EP.y / (lead(EP.t) - EP.t)) |> replace_na(0)),
           fdd = predict(smooth.spline(EP.t, EP.y), deriv = 2)$y[1:(nrow(dat))],
           AUC =pmap_dbl(
             list(EP.t = lag(EP.t), EP.t_cur = EP.t, EP.t_next = lead(EP.t), 
                  EP_y = lag(EP.y), EP_y_cur = EP.y, EP_y_next = lead(EP.y)),
             ~ ifelse(
               is.na(..1) | is.na(..3), 0,  # prüfen, ob genügend Punkte vorhanden sind
               ((..3 - ..1) / 6) * (..2 + 4 * ..5 + ..6) # Simpsonregel-Formel
             )
           )
    ) 
  

# Speichern der Ergebnisse in die entsprechenden Spalten
  for (j in seq_len(min(nrow(dat), num_points))) {
    dev.df1[[paste0("EP.y", j)]][i] <- dat$EP.y[j]
    dev.df1[[paste0("m", j)]][i] <- dat$m[j]
    dev.df1[[paste0("fdd", j)]][i] <- dat$fdd[j]
    dev.df1[[paste0("AUC", j)]][i] <- dat$AUC[j]
  }
  
     # Speichern der Ergebnisse in die entsprechenden Spalten
    # idx <- seq_len(min(nrow(dat), num_points))
    # dev.df1[i, paste0("EP.y", idx)] <- dat$EP.y[idx]
    # dev.df1[i, paste0("m", idx)] <- dat$m[idx]
    # dev.df1[i, paste0("fdd", idx)] <- dat$fdd[idx]
    # dev.df1[i, paste0("AUC", idx)] <- dat$AUC[idx]
  
 }
#}  

saveRDS(dev.df1, "dev_df1.rds")




# Bestimmt die Anzahl der Kerne für R
#num_workers <- max(1, availableCores() - 2)

# Plan für die parallele Verarbeitung
#plan(multisession, workers = 10)

# Parallelisierte Verarbeitung aller Chromatogramme
#future_lapply(seq_len(nrow(dev.df1)), fun.proc.ep)



# Apply the function to each row of the data frame using lapply
# dev.df2 <- lapply(seq_len(nrow(dev.df1)), function(i) {
#   fun.proc.ep(dev.df1[i, , drop = FALSE], hex_column = "curve", num_points = num_points)
# })

# Combine the list of rows back into a single data frame
#dev.df2 <- do.call(rbind, dev.df2)


# Multi-Threading abschließen
#plan(sequential) # Zurück zur Standardverarbeitung

# Ergebnis: EPcsv1 enthält nun die neuen Spalten

```

### Überblick über die Daten {.justify}
```{r skimEPdata, echo=TRUE, eval=TRUE,message=FALSE, warning=FALSE}
#| label: tbl-skimEPPeakData
#| tbl-cap: EP-Daten mit Klassifikation und Features

dev.df1 <- readRDS("dev_df1.rds")

my_skim <- skim_with(base = sfl(
  #Datentyp = skim_type,
  #Variable = skim_variable,
  fehlend = n_missing,
  komplett = n_complete,
  n = length
))
ft <-  flextable(my_skim(dev.df1))|> 
  set_header_labels(skim_type = "Datentyp",
                    skim_variable = "Variable",
                    n_missing = "Fehlende Werte",
                    n = "Anzahl",
                    numeric.mean = "Mittelwert",
                    numeric.sd = "Standardabweichung",
                    numeric.p25 = "25%-Quantil",
                    numeric.p50 = "50%-Quantil",
                    numeric.p75 = "75%-Quantil",
                    numeric.p0 = "Minimum",
                    numeric.p100 = "Maximum",
                    numeric.hist = "Histogramm",
                    factor.n_unique =" Anzahl Faktoren") |>
  autofit() |>
  bold(part = "header") |>
  align(align = "center", part = "all")
ft
```

:::

## Datenvorbereitung für die Modellierung {.justify}

::: {.panel-tabset}

### Auswahl der relevanten Features {.justify}
In der vorliegenden Arbeit möchte ich, wenigstens in einem ersten Schritt auf ein Model zur Unterstützung der Gammopathie-Erkennung fokusieren. Diese werden vorallem durch aberante Peaks in den beta1-, beta2- und gamma-Fraktionen erkannt. 
Daher wähle ich zunächste die Signalhöhen (EP.y), die zugehörigen Steigungen (m), die zweite Ableitung (fdd) und die Flächen unter den Peaks (AUC) in den zweiten Hälften der Chromatogramme - also Messpunkte 150 bis 300, als Features für die Modellierung aus.

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Auswahl der relevanten Features
# Subset des DataFrames für die Modellierung
dev.df1 <- readRDS("dev_df1.rds")
dev.sub <- dev.df1 |> 
  select( ID, 
          Klassifizierung, 
          EP.y150:EP.y300, 
          m150:m300, 
          fdd150:fdd300, 
          AUC150:AUC300)

saveRDS(dev.sub, "dev_sub.rds")


```

### Überblick über die Daten {.justify}

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: tbl-skimEPdata
#| tbl-cap: EP-Daten mit Klassifikation und Features

EPdata <- readRDS("dev_sub.rds")

ft <-  flextable(my_skim(EPdata))|> 
  set_header_labels(skim_type = "Datentyp",
                    skim_variable = "Variable",
                    n_missing = "Fehlende Werte",
                    n = "Anzahl",
                    numeric.mean = "Mittelwert",
                    numeric.sd = "Standardabweichung",
                    numeric.p25 = "25%-Quantil",
                    numeric.p50 = "50%-Quantil",
                    numeric.p75 = "75%-Quantil",
                    numeric.p0 = "Minimum",
                    numeric.p100 = "Maximum",
                    numeric.hist = "Histogramm",
                    factor.n_unique =" Anzahl Faktoren") |>
  autofit() |>
  bold(part = "header") |>
  align(align = "center", part = "all")
ft


```

:::

### Aufteilen der Daten in Trainings-, Validierungs- und Testdaten {.justify}
Ich versuche schon an diesem Punkt mit den Schritten des "Tidymodeling" zu arbeiten .

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
library(tidymodels)


# Aufteilen der Daten in 60% Trainings-, 20% Validierungs- und  20% Testdaten
set.seed(8173)
EP.split <- EPdata |> 
  initial_validation_split(prop = c(0.6, 0.2), strata = Klassifizierung)


saveRDS(EP.split, "EP_split.rds")


```

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
EPtraining <- training(EP.split)
saveRDS(EPtraining, "EPtraining.rds")

EPvalidation <- validation(EP.split)
saveRDS(EPvalidation, "EPvalidation.rds")

#EPtesting <- testing(EP.split)
```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: tbl-EPsplit
#| tbl-cap: Aufteilung der Daten in Trainings-, Validierungs- und Testdaten

EP.split <- readRDS("EP_split.rds")
EP.split


```

## 06.01.2025 Bereinigung der EP-Befunde und Klassifizierung {.justify}

#### Korrektur von *"unauffällig"*
```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Tokenisierung der EP-Befundtexte
library(tidytext)
library(stringr)

Bef.EP.freq <- EPcsv |>
  mutate(
    Bef.EP_cleaned = gsub("\\([A-Za-zäöüÄÖÜ]+(/[A-Za-zäöüÄÖÜ]+)?\\)", "", Bef.EP),
    Bef.EP_cleaned = gsub("[0-9]+", "", Bef.EP_cleaned),
    #Bef.EP_cleaned = str_replace_all(Bef.EP_cleaned, monate, ""),
    Bef.EP_cleaned = str_replace_all(Bef.EP_cleaned, "[[:punct:]]", ""),
    Bef.EP_cleaned = str_replace_all(Bef.EP_cleaned, " +", " "),
    Bef.EP_cleaned = str_replace_all(Bef.EP_cleaned, "-", ""),
    Bef.EP_cleaned = str_squish(Bef.EP_cleaned)
  ) |> 
  unnest_tokens(Bef.EP_cleaned, Bef.EP_cleaned, token = "words", to_lower =TRUE, drop = FALSE) |> 
  count(Bef.EP_cleaned, sort = TRUE) |> 
  arrange(Bef.EP_cleaned)|> 
  mutate(Distanz = stringdist("unauffällig", Bef.EP_cleaned, method = "lv")) |> 
  filter(Distanz < 4) |> 
  pull(Bef.EP_cleaned)

EP.unauf <- paste0("\\b(", paste(Bef.EP.freq, collapse = "|"), ")\\b")



```

### Klassifizierung der EP-Befunde
```{r, echo=TRUE, eval=F, message=FALSE, warning=FALSE}
dev.df1 <- readRDS("dev_df1.rds")


# Klassifizierung der EP-Befunde
dev.df1 <- dev.df1 |> 
  mutate(
    EPKlassifizierung = case_when(
      grepl(EP.unauf, Bef.EP, ignore.case = T) ~ "unauffällig",
      TRUE ~ "suspicous"
    ) |> 
    as.factor()
  )

saveRDS(dev.df1, "dev_df1.rds")

```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: tbl-EPBclass_distribution
#| tbl-cap: Verteilung der EP-Befund-Klassifizierungen

dev.df1 <- readRDS("dev_df1.rds")

ft <- dev.df1 |> 
  count(EPKlassifizierung) |> 
  flextable() |>
  width(j = 1, width = 3.5)
ft

```

### 06.01.2025 EP-Befund-Klassifizierung: Auswahl der relevanten Features {.justify}
In der vorliegenden Arbeit möchte ich, wenigstens in einem ersten Schritt auf ein Model zur Unterstützung der Gammopathie-Erkennung fokusieren. Diese werden vorallem durch aberante Peaks in den beta1-, beta2- und gamma-Fraktionen erkannt. 
Daher wähle ich zunächste die Signalhöhen (EP.y), die zugehörigen Steigungen (m), die zweite Ableitung (fdd) und die Flächen unter den Peaks (AUC) in den zweiten Hälften der Chromatogramme - also Messpunkte 150 bis 300, als Features für die Modellierung aus.

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Auswahl der relevanten Features
# Subset des DataFrames für die Modellierung
# dev.df1 <- readRDS("dev_df1.rds")
dev.sub.EP <- dev.df1 |> 
  select( ID, 
          EPKlassifizierung, 
          #EP.y150:EP.y300, 
          #m150:m300, 
          fdd150:fdd300, 
          #AUC150:AUC300
          )

saveRDS(dev.sub.EP, "dev_sub_EP.rds")

dev.sub.EP.mfdd <- dev.df1 |> 
  select( ID, 
          EPKlassifizierung, 
          #EP.y150:EP.y300, 
          m150:m300, 
          fdd150:fdd300, 
          #AUC150:AUC300
          )

saveRDS(dev.sub.EP.mfdd, "dev_sub_EP_mfdd.rds")

```

### 06.01.2025 Aufteilen der EPKlassifikations-Daten in Trainings-, Validierungs- und Testdaten {.justify}


```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
library(tidymodels)


# Aufteilen der Daten in 60% Trainings-, 20% Validierungs- und  20% Testdaten
set.seed(8173)
EPB.split <- dev.sub.EP |> 
  initial_validation_split(prop = c(0.6, 0.2), strata = EPKlassifizierung)


saveRDS(EPB.split, "EPB_split.rds")


```

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
EPBtraining <- training(EPB.split)
saveRDS(EPBtraining, "EPBtraining.rds")

EPBvalidation <- validation(EPB.split)
saveRDS(EPBvalidation, "EPBvalidation.rds")

#EPtesting <- testing(EP.split)
```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: tbl-EPBsplit
#| tbl-cap: Aufteilung der Daten in Trainings-, Validierungs- und Testdaten

EPB.split <- readRDS("EPB_split.rds")
EPB.split


```
