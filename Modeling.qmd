---
author: "Oliver Speer"
date: "10.12.2024"
---

::::: columns
::: {.column width="50%"}
:::

::: {.column width="50%"}
```{r version number, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
cat("Version:\n",format(file.info("Modeling.qmd")$mtime,"%d. %B %Y"))
```
:::
:::::

```{r, echo=FALSE, message=FALSE, warning=FALSE}
source("StartUp.R")
StartUpRoutine()
dbDisconnect(con)

```

# Modeling {.unnumbered}


## Auswählen der Modellierungsverfahren {.justify}
- erster Schritt: Random Forest
- zweiter Schritt: logistische Regression mit den wichtigsten Variablen aus dem Random Forest

## Generieren eines Testdesigns {.justify}
### Ziel
Das Modell muss die Klasse *"MGradient"* mit einer Spezifität von 95% und einer Sensitivität von 80% identifizieren.

### Datenaufteilung
Wie in "Tidy Modeling with R" beschrieben, werde ich die Daten in   
- 60% Trainingsdaten  
- 20% Validationsdaten  
- 20% Testdaten   
aufteilen.

### Testmethodik
1) Hyperparamtertuning eines random forest mit 10-facher Kreuzvalidierung (Trainigsdaten)  
evtl an downsampling denken.  

2) Auswahl des besten random forst Models    
Metriken: ROC-AUC, F1-Score, Precision-Recall-AUC  
3) Auswahl der  wichtigsten Variablen (variable importance)  
Untersuchung der Wichtigkeit mit gini importance und permutation importance  
Anzahl der wichtigsten Variablen erreicht 95% der Gesamtwichtigkeit (95% importance score)  
4) Training & Tuning einer logistischen Regression / Lasso-Regression mit diesen wichtigsten   Variablen (Trainingsdaten)    
5) Hyperparamtertuning einer logistischen Regression mit 10-facher Kreuzvalidierung (Trainingsdaten)    
6) Auswahl des besten Regressionsmodels    
Metriken: ROC-AUC, F1-Score, Precision-Recall-AUC  
7) Vergleich des besten random forest und der besten Regression auf den Validationsdaten  
8) Auswahl des sinnvollsten Models (Einsatz im Alltag, IT-ressourcen etc als Entscheidungsgrundlage)
9) Testen des finales Models auf den Test-Daten  
Metriken: ROC-AUC, F1-Score, Precision-Recall-AUC, Confusion Matrix

### Evaluationsmetriken
- Sensitivität und ROC-AUC 
- overall classification accuracy (?)

## Erstellen der Modelle {.justify}
### Random Forest {.justify}
#### Erstes Tuning: recipe, tuning spec, workflow, 10x resempling {.justify}
```{r random forest recipe, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

# Daten einlesen
EPtraining <- readRDS("EPtraining.rds")
#EPvalidation <- readRDS("EPvalidation.rds")

# recipe building
tree.rec <- 
  recipe(Klassifizierung ~ ., data = EPtraining) |>
  update_role(ID, new_role = "ID") 

tree.rec
```



```{r random forest tuning spec, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# prepare data
tree.prep <- prep(tree.rec)
juiced <- juice(tree.prep)

# tune setting
tune.spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) |> 
  set_mode("classification")  |> 
  set_engine("ranger")

tune.spec
```


```{r random forest tuning wf, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# workflow
tune.wf <- workflow() |>
  add_recipe(tree.rec) |>
  add_model(tune.spec)

tune.wf
```



```{r random forest tuning folds, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# cross validation data setting
set.seed(8173)
EPfolds <- vfold_cv(EPtraining, v = 10)
EPfolds
```


```{r random forest tuning, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# doParallel::registerDoParallel()
library(future)
plan(multisession, workers = 10)

# tuning
set.seed(8173)
tune.res <- tune_grid(parallel_over = "resamples",
  tune.wf,
  resamples = EPfolds,
  grid = 20
)



future::plan("sequential")

saveRDS(tune.res, "tune_resRF.rds")
```

#### Auswertung des Tuning {.unnumbered}
```{r figtuningRF, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

tune.res <- readRDS("tune_resRF.rds")

tune.res |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  select(mean, min_n, mtry) |>
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) |>
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")

``` 

#### Fine-Tuning {.unnumbered}
```{r finetuning grid, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# cross validation data setting
set.seed(8173)
EPfolds <- vfold_cv(EPtraining, v = 5)
EPfolds

rf.grid <- grid_regular(
  mtry(range = c(30, 50)),
  min_n(range = c(3, 5)),
  levels = 3
)

rf.grid
```


```{r finetuning, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
library(future)
plan(multisession, workers = 5)

set.seed(8173)
regular.res <- tune_grid(
  tune.wf,
  resamples = EPfolds,
  grid = rf.grid,
  parallel_over = "resamples",
  control = control_grid(verbose = TRUE)
)

plan(sequential)

saveRDS(regular.res, "regular_resRF.rds")
```


```{r figfinetuning, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
regular.res <- readRDS("regular_resRF.rds")
regular.res |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(min_n = factor(min_n)) |>
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "AUC")
```

#### Auswahl des besten Modells {.unnumbered}
```{r bestmodelRF, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
best.auc <- select_best(regular.res, metric = "roc_auc")
best.auc

final.rf <- finalize_model(tune.spec, best.auc)

saveRDS(final.rf, "final_rf.rds")

final.rf
```

#### Die wichtigsten Variablen {.unnumbered}
```{r important variables, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# variable importance
library(vip)

final.rf.imp <- final.rf |> 
  set_engine("ranger", importance = "permutation") |>
  fit(Klassifizierung ~ ., 
      data = juice(tree.prep) |> select(-ID)
      )#|>

saveRDS(final.rf.imp, "final_rf_imp.rds")
```


```{r fig important variables, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-important-variables
#| fig.cap: Wichtigste Variablen
#| 
final.rf.imp <- readRDS("final_rf_imp.rds")
library(vip)
vip(final.rf.imp, geom = "point", num_features = 40)
```

<!-- ```{r random forest, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- # Modell Definition -->
<!-- rf.mod <-  -->
<!--   rand_forest(trees = 1000) |>  -->
<!--   set_engine("ranger") |> -->
<!--   set_mode("classification") -->

<!-- # Modell Training -->
<!-- set.seed(8173) -->
<!-- rf.fit <-  -->
<!--   rf.mod |>  -->
<!--   fit(Klassifizierung ~ ., data = EPtraining) -->
<!-- rf.fit -->
<!-- saveRDS(rf.fit, "rf_fit.rds") -->
<!-- rf.fit <- readRDS("rf_fit.rds") -->

<!-- ``` -->
<!-- ### resampling {.justify} -->
<!-- ```{r resampling, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- # resampling -->
<!-- EPvalidation <- readRDS("EPvalidation.rds") -->
<!-- set.seed(8173) -->
<!-- folds <- vfold_cv(EPvalidation, v = 10) -->
<!-- folds -->

<!-- # workflow Definition -->
<!-- rf.wf <- workflow() |> -->
<!--   add_model(rf.mod) |> -->
<!--   add_formula(Klassifizierung ~ .) -->




<!-- ```  -->
<!-- ## Bewerten des Modells {.justify} -->

<!-- ### Sensitivität und ROC-AUC {.justify} -->
<!-- ```{r sensitivity, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- # Sensitivität und ROC-AUC -->
<!-- # resampling -->
<!-- rf.res <-  -->
<!--   rf.wf |> -->
<!--   fit_resamples(resamples = folds) -->
<!-- saveRDS(rf.res, "rf_res10k.rds") -->
<!-- rf.res <- readRDS("rf_res10k.rds") -->
<!-- #rf.res -->

<!-- collect_metrics(rf.res)  -->



<!-- ``` -->


<!-- ### Tuning {.justify} -->
<!-- ```{r tuning, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE} -->
<!-- # Tuning -->
<!-- tune.spec <- decision_tree( -->
<!--   cost_complexity = tune(), -->
<!--   tree_depth = tune() -->
<!-- ) |>  -->
<!--   set_engine("rpart") |>  -->
<!--   set_mode("classification") -->

<!-- tune.spec -->

<!-- tree.grid <- grid_regular( -->
<!--   cost_complexity(), -->
<!--   tree_depth(), -->
<!--   levels = 5 -->
<!-- ) -->

<!-- tree.grid -->

<!-- set.seed(8173) -->
<!-- tune.wf <- workflow() |> -->
<!--   add_model(tune.spec) |> -->
<!--   add_formula(Klassifizierung ~ .) -->

<!-- tune.res <-  -->
<!--   tune.wf |> -->
<!--   tune_grid( -->
<!--     resamples = folds, -->
<!--     grid = tree.grid#, -->
<!--     #control = control_grid(save_pred = TRUE) -->
<!--   ) -->
<!-- saveRDS(tune.res, "tune_res.rds") -->
<!-- ``` -->


<!-- ```{r figtuning, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- #| label: fig-tuning -->
<!-- #| fig.cap: tree depth und cost complexity tuning -->
<!-- tune.res <- readRDS("tune_res.rds") -->

<!-- tune.res |>  -->
<!--   collect_metrics()  |> -->
<!--   mutate(tree_depth = factor(tree_depth)) |> -->
<!--   ggplot(aes(cost_complexity, mean, color = tree_depth)) + -->
<!--   geom_line(size = 1.5, alpha = 0.6) + -->
<!--   geom_point(size = 2) + -->
<!--   facet_wrap(~ .metric, scales = "free", nrow = 2) + -->
<!--   scale_x_log10(labels = scales::label_number()) + -->
<!--   scale_color_viridis_d(option = "plasma", begin = .9, end = 0) + -->
<!--   theme_minimal() + -->
<!--   theme(legend.position = "bottom") + -->
<!--   labs( -->
<!--     x = "cost complexity", -->
<!--     y = "mean", -->
<!--     color = "tree depth" -->
<!--   ) -->


<!-- ``` -->

<!-- ```{r figtuning1, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- #| label: fig-tuning1 -->
<!-- #| fig.cap: tree depth und cost complexity tuning -->
<!-- tune.res <- readRDS("tune_res.rds") -->

<!-- tune.res |>  -->
<!--   collect_metrics()  |> -->
<!--   mutate(tree_depth = factor(tree_depth)) |> -->
<!--   ggplot(aes(cost_complexity, mean, color = tree_depth)) + -->
<!--   geom_line(size = 1.5, alpha = 0.6) + -->
<!--   geom_point(size = 2) + -->
<!--   facet_wrap(~ .metric, scales = "free", nrow = 2) + -->
<!--   scale_x_log10(labels = scales::label_number()) + -->
<!--   scale_color_viridis_d(option = "plasma", begin = .9, end = 0) -->
<!-- ``` -->


<!-- ```{r tabtuningROC, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- #| label: tab-tuningROC -->
<!-- #| tab.cap: Tuning ROC-AUC Ergebnisse  -->

<!-- tune.res |>  -->
<!--   show_best(metric = "roc_auc") -->
<!-- ``` -->


<!-- ```{r tabtuningAccur, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE} -->
<!-- #| label: tab-tuningAccur -->
<!-- #| tab.cap: Tuning Accuracy Ergebnisse -->
<!-- tune.res |>  -->
<!--   show_best(metric = "accuracy") -->
<!-- ``` -->