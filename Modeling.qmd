---
author: "Oliver Speer"
date: "10.12.2024"
---

::::: columns
::: {.column width="50%"}
:::

::: {.column width="50%"}
```{r version number, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
cat("Version:\n",format(file.info("Modeling.qmd")$mtime,"%d. %B %Y"))
```
:::
:::::

```{r, echo=FALSE, message=FALSE, warning=FALSE}
source("StartUp.R")
StartUpRoutine()
dbDisconnect(con)

```

# Modeling {.unnumbered}


## Auswählen der Modellierungsverfahren {.justify}
- erster Schritt: Random Forest
      - Klassifikation nach iFix Befunden
      - Klassifikation nach EP Befunden
- zweiter Schritt: logistische Regression mit den wichtigsten Variablen aus dem Random Forest

## Generieren eines Testdesigns {.justify}
### Ziel
[Das Modell muss die Klasse *"unauffällig"* mit einer Spezifität von 90% und einer Sensitivität von 100% identifizieren.]{style="color:#0000B8;font-weight:bold"}

### Datenaufteilung
Wie in "Tidy Modeling with R" beschrieben, werde ich die Daten in   
- 60% Trainingsdaten  
- 20% Validationsdaten  
- 20% Testdaten   
aufteilen.

### Testmethodik
1) Hyperparamtertuning eines random forest mit 10-facher Kreuzvalidierung **(Trainigsdaten)**  
evtl an downsampling denken.  

2) Auswahl des besten random forst Models    
Metriken: ROC-AUC & accuracy
3) Auswahl der  wichtigsten Variablen (variable importance)  
Untersuchung der Wichtigkeit mit permutation importance  
Anzahl der wichtigsten Variablen erreicht 95% der Gesamtwichtigkeit (95% importance score)?   
4) Training & Tuning einer logistischen Regression / Lasso-Regression mit diesen wichtigsten   Variablen **(Trainingsdaten)**    
5) Hyperparamtertuning einer logistischen Regression mit 10-facher Kreuzvalidierung **(Trainingsdaten)**    
6) Auswahl des besten Regressionsmodels    
Metriken: ROC-AUC, accuracy  
7) Vergleich des besten random forest und der besten Regression auf den **Validationsdaten**  
8) Auswahl des sinnvollsten Models (Einsatz im Alltag, IT-ressourcen etc als Entscheidungsgrundlage)
9) Testen des finales Models auf den **Test-Daten**  
Metriken: ROC-AUC, accuracy, Sensitivität, Spezifität, PPV, NPV  

[welche Test sollte ich noch berücksichtigen?]{style="color:#0000B8;font-weight:bold"}

### Evaluationsmetriken
- Sensitivität und ROC-AUC 
- overall classification accuracy (?)

## Erstellen der Modelle {.justify}
### Random Forest I {.justify}
#### Erstes Tuning: recipe, tuning spec, workflow, 10x resempling {.justify}
```{r random forest recipe, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

# Daten einlesen
EPtraining <- readRDS("EPtraining.rds")
#EPvalidation <- readRDS("EPvalidation.rds")

# recipe building
tree.rec <- 
  recipe(Klassifizierung ~ ., data = EPtraining) |>
  update_role(ID, new_role = "ID") 

tree.rec
```



```{r random forest tuning spec, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# prepare data
tree.prep <- prep(tree.rec)
juiced <- juice(tree.prep)

# tune setting
tune.spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) |> 
  set_mode("classification")  |> 
  set_engine("ranger", num.threads = 2)

tune.spec
```


```{r random forest tuning wf, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# workflow
tune.wf <- workflow() |>
  add_recipe(tree.rec) |>
  add_model(tune.spec)



tune.wf
```



```{r random forest tuning folds, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# cross validation data setting
set.seed(8173)
EPfolds <- vfold_cv(EPtraining, v = 10)
EPfolds
```


```{r random forest tuning, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# doParallel::registerDoParallel()
library(future)
plan(multisession, workers = 10)

# tuning
set.seed(8173)
tune.res <- tune_grid(parallel_over = "resamples",
  tune.wf,
  resamples = EPfolds,
  grid = 20
)



future::plan("sequential")

saveRDS(tune.res, "tune_resRF.rds")
```

#### Auswertung des Tuning {.unnumbered}
```{r figtuningRF, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

tune.res <- readRDS("tune_resRF.rds")

tune.res |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  select(mean, min_n, mtry) |>
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) |>
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")

``` 

#### Fine-Tuning {.unnumbered}
```{r finetuning grid, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# cross validation data setting
set.seed(8173)
EPfolds <- vfold_cv(EPtraining, v = 5)
EPfolds

rf.grid <- grid_regular(
  mtry(range = c(30, 50)),
  min_n(range = c(3, 5)),
  levels = 3
)

rf.grid
```


```{r finetuning, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
library(future)
plan(multisession, workers = 5)

set.seed(8173)
regular.res <- tune_grid(
  tune.wf,
  resamples = EPfolds,
  grid = rf.grid,
  parallel_over = "resamples",
  control = control_grid(verbose = TRUE)
)

plan(sequential)

saveRDS(regular.res, "regular_resRF.rds")
```


```{r figfinetuning, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
regular.res <- readRDS("regular_resRF.rds")
regular.res |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(min_n = factor(min_n)) |>
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "AUC")
```

#### Auswahl des besten Modells {.unnumbered}
```{r bestmodelRF, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
best.auc <- select_best(regular.res, metric = "roc_auc")
best.auc

final.rf <- finalize_model(tune.spec, best.auc)

saveRDS(final.rf, "final_rf.rds")

final.rf
```

#### Die wichtigsten Variablen {.unnumbered}
```{r important-variables, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# variable importance
library(vip)

final.rf.imp <- final.rf |> 
  set_engine("ranger", importance = "permutation") |>
  fit(Klassifizierung ~ ., 
      data = juice(tree.prep) |> select(-ID)
      )#|>

saveRDS(final.rf.imp, "final_rf_imp.rds")
```


```{r fig-important-variables, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-important-variables
#| fig.cap: Wichtigste Variablen
#| 
final.rf.imp <- readRDS("final_rf_imp.rds")
library(vip)
vip(final.rf.imp, geom = "point", num_features = 40)
```

### Random Forest II {.justify}
Mit den Erkentnissen aus dem ersten Random Forest Modell, wird ein zweites Modell erstellt. Dieses basiert auf der Klassifizierung in "unauffällig" und "suspicous" auf Grundlage der EP Befunde. Ausserdem werden - nach der Abbildung oben - als Features nur die 2ten Ableitungen verwendet.

#### Erstes Tuning: recipe, tuning spec, workflow, 10x resempling {.justify}
```{r random forest II recipe, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

# Daten einlesen
EPBtraining <- readRDS("EPBtraining.rds")
#EPvalidation <- readRDS("EPvalidation.rds")

# recipe building
EPB.rec <- 
  recipe(EPKlassifizierung ~ ., data = EPBtraining) |>
  update_role(ID, new_role = "ID") 


```



```{r random forest II tuning spec, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# prepare data
EPB.prep <- prep(EPB.rec)
juiced <- juice(EPB.prep)

# tune setting
tune.spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) |> 
  set_mode("classification")  |> 
  set_engine("ranger", num.threads = 2)

tune.spec
```


```{r random forest II tuning wf, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# workflow
tune.wf <- workflow() |>
  add_recipe(EPB.rec) |>
  add_model(tune.spec)

tune.wf
```



```{r random forest II tuning folds, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# cross validation data setting
set.seed(8173)
EPfolds <- vfold_cv(EPBtraining, v = 5)
EPfolds
```


```{r random forest II tuning, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# doParallel::registerDoParallel()
library(future)
plan(multisession, workers = 5)

# tuning
set.seed(8173)
tune.EPB.res <- tune_grid(parallel_over = "resamples",
  tune.wf,
  resamples = EPfolds,
  metrics = metric_set(roc_auc, sens, spec, npv),
  grid = 10
)



future::plan("sequential")

saveRDS(tune.EPB.res, "tune_npv_EPB_resRF.rds")
```

#### Auswertung des EPB Tuning {.unnumbered}
```{r figtuningRFII, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: figtuningRFII
#| fig.cap: Tuning des Random Forest Modells für die EP Befunde

tune.EPB.res <- readRDS("tune_EPB_resRF.rds")

tune.EPB.res |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  select(mean, min_n, mtry) |>
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) |>
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")

``` 
#### Fine-Tuning {.unnumbered}
```{r finetuningII grid, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(tidymodels)
EPBtraining <- readRDS("EPBtraining.rds")
# cross validation data setting
set.seed(8173)
EPfolds <- vfold_cv(EPBtraining, v = 5)
EPfolds

rf.grid <- grid_regular(
  mtry(range = c(1, 12)),
  min_n(range = c(1, 9)),
  levels = 4
)

rf.grid
```
#### Tune Spezifikationen und Workflow 

```{r specs finetuning II, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# tune setting
tune.spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) |> 
  set_mode("classification")  |> 
  set_engine("ranger", num.threads = 2)

#tune.spec

# workflow
tune.wf <- workflow() |>
  add_recipe(EPB.rec) |>
  add_model(tune.spec)

tune.wf

```


```{r finetuningII, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
library(future)

plan(multisession, workers = 5)

set.seed(8173)
regular.EPB.res <- tune_grid(
  tune.wf,
  resamples = EPfolds,
  metrics = metric_set(roc_auc, sens, spec, npv),
  grid = rf.grid,
  parallel_over = "resamples",
  control = control_grid(verbose = TRUE)
)

plan(sequential)

saveRDS(regular.EPB.res, "regular_EPB_resRF_npv.rds")

```


```{r figfinetuning II1, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: figfinetuningII1
#| fig.cap: 1. Feintuning des Random Forest Modells für die EP Befunde
regular.res <- readRDS("regular_EPB_resRF1.rds")
regular.res |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(min_n = factor(min_n)) |>
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "AUC")
```

```{r figfinetuning II2, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: figfinetuningII2
#| fig.cap: 2. Feintuning des Random Forest Modells für die EP Befunde
#| 
regular.res <- readRDS("regular_EPB_resRF.rds")
regular.res |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  mutate(min_n = factor(min_n)) |>
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "AUC")
```

```{r figfinetuning II2npv, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: figfinetuningII2npv
#| fig.cap: 2. Feintuning des Random Forest Modells für die EP Befunde Basis NPV
#| 
regular.res <- readRDS("regular_EPB_resRF_npv.rds")
regular.res |>
  collect_metrics() |>
  filter(.metric == "npv") |>
  mutate(min_n = factor(min_n)) |>
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "NPV")
```

#### Auswahl des besten Modells {.unnumbered}
```{r bestmodelRFII, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
regular.res <- readRDS("regular_EPB_resRF.rds")
best.auc <- select_best(regular.res, metric = "roc_auc")
best.auc

final.rf <- finalize_model(tune.spec, best.auc)

saveRDS(final.rf, "final_EPB_rf.rds")

final.rf
```

```{r bestmodelRFIInpv, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
regular.res <- readRDS("regular_EPB_resRF_npv.rds")
best.npv <- select_best(regular.res, metric = "npv")
best.npv

final.rf.npv <- finalize_model(tune.spec, best.npv)

saveRDS(final.rf.npv, "final_EPB_rf_npv.rds")

final.rf.npv
```

#### Die wichtigsten Variablen {.unnumbered}
```{r important-variablesII, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# variable importance
library(vip)
plan(multisession, workers = 5)

final.rf.imp <- final.rf |> 
  set_engine("ranger", importance = "permutation", num.threads = 2) |>
  fit(EPKlassifizierung ~ ., 
      data = juice(EPB.prep) |> select(-ID)
      )#|>

plan(sequential)

saveRDS(final.rf.imp, "final_EPB_rf_imp.rds")
```


```{r fig-important-variablesII, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-EPBimportant-variables
#| fig.cap: Wichtigste Variablen
#| 
final.rf.imp <- readRDS("final_EPB_rf_imp.rds")
library(vip)
vip(final.rf.imp, geom = "point", num_features = 50)
```

```{r tab-important-variablesII, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: tab-EPBimportant-variables
#| tab.cap: Wichtigste Variablen des Random Forest Modells
#| 
final.rf.imp <- readRDS("final_EPB_rf_imp.rds")
library(vip)
var.imp <- vi(final.rf.imp) |> 
  arrange(desc(Importance)) |> 
  head(50)

ft <-  var.imp |> 
  flextable()
ft

```


### Logistische Regression {.justify}

#### Model-Fit mit den Top 50: recipe, tuning spec, workflow, 10x resempling {.justify}

```{r logistic-regression, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Daten einlesen
EPBtraining <- readRDS("EPBtraining.rds")

# formula
EPB.formula <- as.formula(paste("EPKlassifizierung ~", paste(var.imp$Variable, collapse = " + ")))




# Funktion zur parallelisierten Durchführung der Cross-Validation einer logistischen Regression (glm binomial) mit Rezept und Modellspezifikation
# fun.cv.log.reg <- function(formula, training_data, folds = 10, seed = 8173, workers = 10) {
#   set.seed(seed)
# 
#   # Cross-Validation-Splits erstellen
#   cv_folds <- vfold_cv(training_data, v = folds)
# 
#   # Rezept erstellen
#   log_rec <-
#     recipe(formula, data = training_data)
# 
#   # Modellspezifikationen
#   log_spec <-
#     logistic_reg() |>
#     set_engine(
#       engine = "glm",
#       family = binomial
#     ) |>
#     set_mode("classification")
# 
#   # Workflow erstellen
#   log_wf <- workflow() |>
#     add_recipe(log_rec) |>
#     add_model(log_spec)
# 
#   # Hilfsfunktion zum Extrahieren von Modellen
#   get_model <- function(x) {
#     extract_fit_parsnip(x) |> tidy()
#   }
# 
#   # Parallele Verarbeitung einrichten
#   plan(multisession, workers = workers)
# 
#   # Cross-Validation ausführen
#   set.seed(seed)
#   log_res <- log_wf |>
#     fit_resamples(
#       resamples = cv_folds,
#       parallel_over = "resamples",
#       metrics = metric_set(accuracy, roc_auc, sens, spec, ppv, npv),
#       control = control_resamples(
#         save_pred = TRUE,
#         extract = get_model
#       )
#     )
# 
#   return(log_res)
# }
# 
# save(fun.cv.log.reg, file = "fun.cv.log.reg.RData")
# Beispielaufruf der Funktion
# Ergebnis <- perform_cross_validation(formula = EPB.formula, training_data = EPBtraining)

source("fun.cv.log.reg.R")

log.res <-  fun.cv.log.reg(EPB.formula, EPBtraining)

saveRDS(log.res, "log_res.rds")
  


```



##### Performance Metrics der logistischen Regression
```{r performance-logistic-regression, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
log.res <- readRDS("log_res.rds")
log.res |>
  collect_metrics(summarize = TRUE) |>    
  mutate(
    metric = .metric,
    mean = round(mean, 2),
    sd = round(std_err, 3)
  ) |> 
  select(metric, mean, n, sd) |>
  flextable()
```
#### Model-Fit mit allen fdd features {.justify}
```{r logistic-regression-all, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Daten einlesen
EPBtraining <- readRDS("EPBtraining.rds")

# formula
EPB.formula <- as.formula(paste("EPKlassifizierung ~", paste(names(EPBtraining)[!(names(EPBtraining) %in% c("ID", "EPKlassifizierung"))]
, collapse = " + ")))

log.res.all <-  fun.cv.log.reg(EPB.formula, EPBtraining)

saveRDS(log.res.all, "log_res_all.rds")

```

##### Performance Metrics der logistischen Regression
```{r performance-logistic-regression-all, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
log.res.all <- readRDS("log_res_all.rds")
log.res.all |> 
  collect_metrics(summarize = TRUE) |> 
  mutate(
    metric = .metric,
    mean = round(mean, 2),
    sd = round(std_err, 3)
  ) |> 
  select(metric, mean, n, sd) |>
  flextable()

```


### Lasso Regression {.justify}
#### Model-Fit mit den Top 50: recipe, tuning spec, workflow, 10x resempling {.justify}
```{r lasso-regression, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Daten einlesen
EPBtraining <- readRDS("EPBtraining.rds")

# cross validation
set.seed(8173)
EPboot <- bootstraps(EPBtraining, strata = EPKlassifizierung)

# formula
EPB.formula <- as.formula(paste("EPKlassifizierung ~", paste(names(EPBtraining)[!(names(EPBtraining) %in% c( "EPKlassifizierung"))]
, collapse = " + ")))

EPB.formula

# recipe
lasso.rec <- 
  recipe(EPB.formula, data = EPBtraining) |>
  update_role(ID, new_role = "ID")
    

# model specification
lasso.spec <- 
  logistic_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet") 
lasso.spec

# grid
lasso.grid <- grid_regular(
  penalty(),
  levels = 50
)
lasso.grid

# workflow
lasso.wf <- workflow() |>
  add_recipe(lasso.rec) |>
  add_model(lasso.spec)
lasso.wf
```


```{r lasso-regression-tuning, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# tuning
plan(multisession, workers = 10)

set.seed(8173)
lasso.res <- tune_grid(
  lasso.wf,
  resamples = EPboot,
  grid = lasso.grid,
  parallel_over = "resamples",
  metrics = metric_set(roc_auc, accuracy, kap, sens, spec, ppv, npv, f_meas),
  control = control_grid(verbose = TRUE,
                         save_pred = TRUE,
                         parallel_over = "resamples"
                         )
                      )

plan(sequential)

saveRDS(lasso.res, "lasso_res.rds")
```

##### Performance Metrics der Lasso Regression
```{r performance-lasso-regression, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
lasso.res <- readRDS("lasso_res.rds")
lasso.res |> 
  collect_metrics(summarize = TRUE) |> 
    mutate(
    metric = .metric,
    mean = round(mean, 2),
    sd = round(std_err, 3)
  ) |> 
  select(metric, mean, n, sd) |>
  flextable()

```


```{r fig-performance-lasso-regression, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-lasso-regression-tuning
#| fig.cap: Tuning der Lasso Regression
 lasso.res |> 
  collect_metrics() |> 
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")
```

#### Auswahl des besten Modells {.unnumbered}
##### ROC-AUC
```{r bestmodelLasso, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
lowest.roc <- lasso.res  |> 
  select_best(metric = "roc_auc")

saveRDS(lowest.roc, "lowest_roc.rds")

final.lasso <- finalize_workflow(
  lasso.wf,
  lowest.roc
)

final.lasso

saveRDS(final.lasso, "final_lasso.rds")
```


```{r fig-lasso-importance, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-lasso-importance
#| fig.cap: Wichtigste Variablen der Lasso Regression

final.lasso <- readRDS("final_lasso.rds")
lowest.roc <- readRDS("lowest_roc.rds")

library(vip)

final.lasso |>
  fit(EPBtraining) |>
  pull_workflow_fit() |>
  vi(lambda = lowest.roc$penalty) |>
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)
  ) |>
  slice_max(order_by = Importance, n = 40) |>
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

```

##### NPV
```{r bestmodelLasso-npv, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
show_best(lasso.res, metric = "npv")

best.npv <- lasso.res  |> 
  select_best(metric = "npv")

saveRDS(best.npv, "best_npv.rds")

final.lasso.npv <- finalize_workflow(
  lasso.wf,
  best.npv
)


saveRDS(final.lasso.npv, "final_lasso_npv.rds")

final.lasso.npv
```


```{r fig-lasso-importance-npv, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: fig-lasso-importance-npv
#| fig.cap: Wichtigste Variablen der Lasso Regression nach NPV-Tuning

final.lasso.npv <- readRDS("final_lasso_npv.rds")
best.npv <- readRDS("best_npv.rds")

library(vip)

final.lasso.npv |>
  fit(EPBtraining) |>
  pull_workflow_fit() |>
  vi(lambda = best.npv$penalty) |>
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)
  ) |>
  slice_max(order_by = Importance, n = 20) |>
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

```



#### Fit und Vorhersagen mit der Lasso-Regression nach Tuning für ROC-AUC {.unnumbered}
```{r fit-lasso-regression, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Fit des finalen Modells
EPBvalidation <- readRDS("EPBvalidation.rds") 
final.lasso <- readRDS("final_lasso.rds")

lasso.fit <- fit(final.lasso, data = EPBtraining)

saveRDS(lasso.fit, "lasso_fit.rds")

# Vorhersagen
lasso.pred <- predict(lasso.fit, new_data = EPBvalidation, type = "prob") |> 
   bind_cols(predict(lasso.fit, new_data = EPBvalidation, type = "class")) |>
   bind_cols(EPBvalidation)
saveRDS(lasso.pred, "lasso_pred.rds")
```


```{r tab-metrics-lasso-regression, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: tab-metrics-lasso-regression
#| tab.cap: Metriken der Lasso Regression nach Tuning für ROC-AUC

lasso.pred <- readRDS("lasso_pred.rds")

# Calculate metrics
ft <- bind_rows(
  roc_auc(data = lasso.pred, truth = EPKlassifizierung, .pred_unauffällig),
  accuracy(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  kap(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  sensitivity(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  specificity(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  ppv(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  npv(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  f_meas(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class)
) |> 
  mutate(
    metric = .metric,
    estimate = round(.estimate, 2)
  ) |>
  select(metric, estimate) |>
  flextable()
ft
```

#### Fit und Vorhersagen mit der Lasso-Regression nach Tuning für NPV {.unnumbered}
```{r fit-lasso-regression-npv, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Fit des finalen Modells
EPBvalidation <- readRDS("EPBvalidation.rds") 
final.lasso.npv <- readRDS("final_lasso_npv.rds")

lasso.fit <- fit(final.lasso.npv, data = EPBtraining)

saveRDS(lasso.fit, "lasso_fit_npv.rds")

# Vorhersagen
lasso.pred <- predict(lasso.fit, new_data = EPBvalidation, type = "prob") |> 
   bind_cols(predict(lasso.fit, new_data = EPBvalidation, type = "class")) |>
   bind_cols(EPBvalidation)
saveRDS(lasso.pred, "lasso_pred_npv.rds")
```


```{r tab-metrics-lasso-regression-npv, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| label: tab-metrics-lasso-regression-npv
#| tab.cap: Metriken der Lasso Regression nach Tuning für NPv

lasso.pred <- readRDS("lasso_pred_npv.rds")

# Calculate metrics
ft <- bind_rows(
  roc_auc(data = lasso.pred, truth = EPKlassifizierung, .pred_unauffällig),
  accuracy(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  kap(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  sensitivity(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  specificity(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  ppv(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  npv(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class),
  f_meas(data = lasso.pred, truth = EPKlassifizierung, estimate = .pred_class)
) |> 
  mutate(
    metric = .metric,
    estimate = round(.estimate, 2)
  ) |>
  select(metric, estimate) |>
  flextable()
ft
```